<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>翻译A Survey on Vision Transformer | lijinhao's blog</title><meta name="keywords" content="DL"><meta name="author" content="lijinhao"><meta name="copyright" content="lijinhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer是一种基于自注意力机制的深度神经网络，首先应用于自然语言处理领域。受到其强大的表示能力的启发，研究人员正在寻找将Transformer应用于计算机视觉任务的方法。在各种各样的可视化基准测试中，基于Transformer的模型的性能与其他类型的网络相似，甚至更好，比如卷积和循环网络。"><meta property="og:type" content="article"><meta property="og:title" content="翻译A Survey on Vision Transformer"><meta property="og:url" content="https://lijinhao.site/2021/12/14/%E7%BF%BB%E8%AF%91%E3%80%8AA%20Survey%20on%20Vision%20Transformer%E3%80%8B/index.html"><meta property="og:site_name" content="lijinhao&#39;s blog"><meta property="og:description" content="Transformer是一种基于自注意力机制的深度神经网络，首先应用于自然语言处理领域。受到其强大的表示能力的启发，研究人员正在寻找将Transformer应用于计算机视觉任务的方法。在各种各样的可视化基准测试中，基于Transformer的模型的性能与其他类型的网络相似，甚至更好，比如卷积和循环网络。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://gitee.com/leekinghou/image/raw/master/img/20211214202024.png"><meta property="article:published_time" content="2021-12-13T16:00:00.000Z"><meta property="article:modified_time" content="2021-12-14T12:26:03.443Z"><meta property="article:author" content="lijinhao"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Transformer"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/leekinghou/image/raw/master/img/20211214202024.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lijinhao.site/2021/12/14/%E7%BF%BB%E8%AF%91%E3%80%8AA%20Survey%20on%20Vision%20Transformer%E3%80%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"Copy successfully",error:"Copy error",noSupport:"The browser does not support"},relativeDate:{homepage:!1,post:!1},runtime:"days",date_suffix:{just:"Just",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"翻译A Survey on Vision Transformer",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2021-12-14 20:26:03"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));GLOBAL_CONFIG_SITE.isHome&&/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom@1.0.0.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/portrait.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">38</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">21</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i> <span>清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i> <span>照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">lijinhao's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i> <span>清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i> <span>照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">翻译A Survey on Vision Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-12-13T16:00:00.000Z" title="Created 2021-12-14 00:00:00">2021-12-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-12-14T12:26:03.443Z" title="Updated 2021-12-14 20:26:03">2021-12-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Deep-Learning/">Deep Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="翻译A Survey on Vision Transformer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="对于视觉Transformers的调查"><a href="#对于视觉Transformers的调查" class="headerlink" title="对于视觉Transformers的调查"></a>对于视觉Transformers的调查</h1><center>Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,</center><center>Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao Fellow, IEEE</center><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Transformer是一种基于自注意力机制的深度神经网络，首先应用于自然语言处理领域。受到其强大的表示能力的启发，研究人员正在寻找将Transformer应用于计算机视觉任务的方法。在各种各样的可视化基准测试中，基于Transformer的模型的性能与其他类型的网络相似，甚至更好，比如卷积和循环网络。Transformer由于其高性能和不需要人为定义的归纳偏置，越来越受到计算机视觉界的关注。本文对这些Visual Transformer模型进行了分类，并分析了它们的优缺点。我们探讨的主要类别包括主干网络、高/中层次视觉、低层次视觉和视频处理。我们还简要介绍了计算机视觉中的自注意力机制，因为它是Transformer的基本组成部分。此外，我们还总结了有效的Transformer方法，将Transformer推入基于设备的实际应用中。在本文的最后，我们讨论了视觉Transformer面临的挑战，并提出了进一步研究的方向。</p><p><strong>关键词</strong>：Transformer，自注意力机制， 计算机视觉， 高层次视觉， 低层次视觉， 视频。</p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>​ 深度神经网络（DNNs）已经成为当今人工智能（AI）系统的基础构造。不同类型的任务通常涉及不同类型的网络。例如，多层感知（multi-layer perception，MLP）或全连接（full - connected，FC）网络是神经网络的经典类型，它是由多个线性层和非线性激活叠加而成的[128, 129]。卷积神经网络（CNNs）引入了卷积层和池化层，用于处理图像等位移不变数据[83,80]。而循环神经网络（RNNs）利用循环单元来处理顺序数据或时间序列数据[130, 60]。Transformer是一种新型的神经网络。它主要利用自我注意机制[6, 111]来提取内在特征[152]，在人工智能应用中展现广泛的应用潜力。</p><p>​ Transformer首先被应用于自然语言处理（NLP）任务，并取得了显着的改进[152, 34, 11]。例如，Vaswani等人[152]首先提出了仅基于机器翻译和英语选区解析任务的注意机制的Transformer。Devlin等人[34]引入了一种新的语言表示模型，称为BERT（Bidirectional Encoder Representations from Transformers 基于Transformer的双向编码器表示），它将Transformer预先训练到未标记的文本，考虑到每个单词的上下文（它是双向的）。当BERT发表时，它在11个NLP任务中获得了最先进的性能。Brown等人[11]使用1750亿个参数，在45TB压缩明文数据上预先训练了一个名为GPT-3（Generative Pre-trained Transformer 3 ）的大型Transformer 模型。在不需要任何微调的情况下，它在不同类型的下游自然语言任务上实现了强大的性能。这些基于Transformer 的模型具有较强的表示能力，在自然语言处理领域取得了重大突破。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214144830.png" style="zoom:67%"><center>图1：Transformer发展的关键里程碑。 视觉Transformer模型以红色标记</center><p>​ 受Transformer体系结构在自然语言处理领域取得的巨大成功的启发，研究人员最近将Transformer应用到计算机视觉任务中。在视觉应用中，CNN曾被认为是最基本的组件[57, 126]，但如今Transformer显示它是CNN的一个可行的替代方案。Chen等人[21]训练了一个序列转换器来自回归预测像素，在图像分类任务上取得了与cnn相当的结果。另一种视觉Transformer模型是ViT，它将纯Transformer直接应用于图像补丁序列。最近由Dosovitskiy等人提出的[36]，在多个图像识别基准上取得了最先进的性能。除了基本的图像分类，Transformer还被用于解决各种其他计算机视觉问题，包括目标检测[15,193]、语义分割、图像处理和视频理解。由于其卓越的性能，越来越多的研究人员提出了基于Transformer的模型，以改善广泛的视觉任务。</p><p>​ 由于基于Transformer的视觉模型数量迅速增加，跟上新进度的速度越来越困难。因此，当务之急是对现有工程进行调查，这对社会有利。在本文中，我们将对视觉 Transformer器的最新进展进行全面的概述，并讨论进一步改进的潜在方向。为了便于今后对不同主题的研究，我们根据其应用场景对Transformer模型进行分类，如表1所示。主要类别包括骨干网、高/中级别视觉、低级别视觉和视频处理。 <strong>高级视觉处理的是对图像所见内容的解释和使用[150]，中级视觉处理的是如何将这些信息组织成我们所体验到的物体和表面[77]。</strong>在基于dnn的视觉系统中，高、中级视觉之间的差距越来越模糊[194, 102]，因此我们将其作为单一类别来对待。解决这些高/中级视觉任务的Transformer模型的一些例子包括DETR[15]，用于目标检测的变形DETR[193]和用于分割的Max-DeepLab[155]。<strong>低层次图像处理主要是从图像中提取描述(这些描述通常表示为图像本身)[43]。</strong>低层次图像处理的典型应用包括超分辨率、图像去噪和样式转换。目前，只有少数作品[20, 113]在低级视觉中使用变形器，这就产生了进一步研究的需要。另一类是视频处理，它是计算机视觉和基于图像的任务的重要组成部分。由于视频的顺序特性，Transformer天生非常适合用于视频任务[191, 178]，在这些任务中，它的性能开始与传统的cnn和rnn相当。在这里，我们回顾了与基于Transformer的视觉模型相关的工作，以跟踪这一领域的进展。图1显示了视觉Transformer的开发时间表——毫无疑问，未来还会有更多的里程碑。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214154642.png" style="zoom:50%"><p>​ 本文的其余部分组织如下。第二节讨论了标准Transformer的制定和自注意机制。第三部分，我们描述了在自然语言处理中Transformer的方法，因为研究经验可能有助于视觉任务。第四部分是本文的主要部分，主要从主干、高/中层次、低层次和视频任务四个方面对视觉变形模型进行了总结。我们还简要描述了CV和高效Transformer方法的自我注意机制，因为它们与我们的主题密切相关。在最后一部分，我们给出了结论，并讨论了几个研究方向和面临的挑战。在本次调查中，我们主要总结了代表性工作（早期的，开创性的，新颖的，或鼓舞人心的作品），因为 arXiv 上有很多预印本，我们不能在有限的页面中将它们全部概括完整。</p><h2 id="2-Transformer的公式"><a href="#2-Transformer的公式" class="headerlink" title="2. Transformer的公式"></a>2. Transformer的公式</h2><p>​ Transformer[152]首次用于机器翻译任务的神经语言处理（NLP）领域。如图2所示，它由一个编码器模块和一个解码器模块组成，该模块具有多个相同架构的编码器/解码器。每个编解码器由自注意层和前馈神经网络组成，而每个解码器还包含一个编解码器注意层。在Transformer 用于翻译句子之前，句子中的每个单词都需要嵌入到$d_{model}$​维的向量中。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214145831.png" style="zoom:50%"><center>图2: vanilla transformer的管道</center><h3 id="2-1-自注意力机制的常规公式"><a href="#2-1-自注意力机制的常规公式" class="headerlink" title="2.1 自注意力机制的常规公式"></a>2.1 自注意力机制的常规公式</h3><p>​ 用于机器翻译的自注意力模块 [225] 通过估计所有位置的注意力分数并根据分数收集相应的嵌入来计算序列中每个位置的响应。 这可以看作是一种非局部过滤操作 [233]、[14]。 我们遵循约定 [233] 来制定自我注意模块。 给定输入信号（例如图像、序列、视频和特征）$X ∈ R^{n×d}$，其中$ n = h × w$（表示特征中的像素数）并且 $d $是通道数，输出信号生成为：<br>$$<br>y_i = {1\over C(X_i)} \sum_{∀j}f(X_i, X_j)g(X_j)\tag{1}<br>$$<br>​ 其中 $x_i ∈ R1×d$ 和$ yi ∈ R_1×d$ 分别表示输入信号 $X$ 和输出信号$ Y $的第$ i $个位置（例如空间、时间和时空）。下标 $j$ 是枚举所有位置的索引，成对函数$ f(·) $计算$ i $和所有$ j $之间的表示关系（例如亲和度）。 函数$ g(·) $计算位置$ j$ 处输入信号的表示，并且响应由因子$ C(x_i)$ 归一化。</p><p>​ 请注意，成对函数 $f(·)$ 有很多选择。 例如，可以使用高斯函数的简单扩展来计算嵌入空间中的相似度。 因此，函数 $f(·) $可以表示为：<br>$$<br>f(X_i, X_j) = e^{\theta(x_i)φ(x_j)^T}\tag{2}<br>$$<br>其中$θ(·)$和$ φ(·) $可以是任何嵌入层，如果我们考虑$θ(·)$， $φ(·) $，$g(·)$以以下线性形式嵌入：$θ(X) = XW_θ, φ(X) = XW_φ,g(X) = XW_g$其中，$W_θ ∈ R_d×d_k,W_φ ∈ R_d×d_k,W_g ∈ R_d×d_v$，归一化参数是 $C(x_i ) = \sum_{∀j}{f(X_i , X_j)}$，等式可以写成：<br>$$<br>y_i = {e^{x_iw_{θ,i} w^T_{φ,j}X^T_j}\over \sum_{j}e^{x_iw_{θ,i} w^T_{φ, j}X^T_j}}X_jW_{g, j} , \tag{3}<br>$$<br>​ 其中 $w_{θ, i} ∈ R^{n × c}$ 是权重矩阵$ W_θ$ 的第$i$ 行。 为一个给定索引$i$, $1 \over f(x_i,x_j)$ 成为沿 $C(x_i)$ 的 softmax 输出维度$j$。 公式可以进一步改写为:<br>$$<br>Y = softmax(XW_θ W_φ^T X)g(X),\tag{4}<br>$$<br>​ 其中 $Y ∈ R_n×c$ 是与 $X$ 相同大小的输出信号。 与查询相比，来自翻译模块的键和值表示 $Q = XW_q, K = XW_k, V = XW_v$，一旦 $W_q = W_θ,W_k = W_φ ,W_v = W_g$，方程。 4 可以表示为：<br>$$<br>Y = softmax(QK_T)V = Attention(Q, K, V), \tag{5}<br>$$<br>为机器翻译提出的自注意力模块 [225] 在某种程度上与前面为计算机视觉提出的非局部过滤操作相同。</p><p>​ 通常，计算机视觉的自注意力模块的最终输出信号会被包装为：<br>$$<br>Z=YW^o+X \tag{6}<br>$$<br>其中$ Y$ 是通过等式生成的。 等式4 如果 $W^o$​ 被初始化为零，这个 self-attention 模块可以被插入到任何现有的模型中，而不会破坏它的初始行为。</p><h3 id="2-2-缩放的点乘自注意力机制"><a href="#2-2-缩放的点乘自注意力机制" class="headerlink" title="2.2 缩放的点乘自注意力机制"></a>2.2 缩放的点乘自注意力机制</h3><p>​ 在自注意力层，输入向量首先转化为三个不同的向量：查询向量$ q$、关键向量$ k$ 和$d_q =d_k =d_v =d_{model} =512$。模型然后将来自不同输入的向量打包成三个不同的矩阵，即 $Q、K 和 V$。 随后，不同输入向量之间的注意力函数计算如下（如图 3 左所示）：</p><ul><li><p>第 1 步：计算不同输入向量之间的分数，其中$S = Q·K^⊤$；</p></li><li><p>第 2 步：对梯度稳定性的分数进行归一化 $S_n = S/√dk$；</p></li><li><p>第 3 步：使用 softmax 将分数转换为概率函数 P = softmax(Sn)；</p></li><li><p>步骤 4：获得 Z = V · P 的加权值矩阵。</p></li></ul><p>​ 该过程可以统一为一个函数：<br>$$<br>Attention(Q, K, V) = softmax(Q · K^T \over \sqrt{d_k} · V \tag{7}<br>$$<br>​ 等式 7 背后的逻辑很简单，第 1 步计算每对不同向量之间的分数，这些分数决定了我们在对当前位置的词进行编码时给予其他词的关注程度。 第 2 步对分数进行归一化以增强梯度稳定性以改进训练，第 3 步将分数转换为概率。 最后，每个值向量乘以概率之和。 较大的向量概率从下层获得额外的关注。解码器模块中的编码器-解码器注意力层与编码器模块中的自注意力层类似，但有以下区别： 键矩阵 $K$ 和值矩阵 $V$ 源自编码器模块，查询矩阵 $Q$ 源自 上一层。</p><p>​ 请注意，前面的过程对每个单词的位置是不变的，这意味着 self-attention 层缺乏捕获单词在句子中的位置信息的能力。 然而，语言中句子的顺序性质要求我们将位置信息合并到我们的编码中。 为了解决这个问题并允许获得单词的最终输入向量，将具有维度$ d_{model} $​的位置编码添加到原始输入嵌入中。 具体来说，位置用以下等式编码<br>$$<br>PE(pos, 2i) = sin(pos / 10000^{2i \over{d_{model}}}) \tag{8}<br>$$</p><p>$$<br>PE(pos, 2i + 1) = cos(pos / 10000^{2i \over{d_{model}}}) \tag{9}<br>$$</p><p>​ 其中$pos$表示单词在句子中的位置，$i$表示位置编码的当前维度。 通过这种方式，位置编码的每个元素都对应一个正弦曲线，它允许变换器模型学习通过相对位置来参与并在推理过程中外推到更长的序列长度。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214161106.png" style="zoom:50%"><center>图 3：（左）自注意力过程。（右）多头注意力。 图像来自[225]。</center><p>**多头自注意力机制 **。多头注意力机制可用于提高普通自注意力层的性能。 请注意，对于给定的参考词，我们在浏览句子时通常希望关注其他几个词。 单头自注意力层限制了我们专注于一个或多个特定位置的能力，而不会同时影响对其他同等重要位置的注意力。 这是通过给注意力层不同的表示子空间来实现的。 具体来说，不同的查询矩阵、键矩阵和值矩阵用于不同的头部，这些矩阵由于随机初始化可以将训练后的输入向量投影到不同的表示子空间中。</p><p>​ 为了更详细地说明这一点，给定一个输入向量和头数$ h$，输入向量首先被转换为三个不同的向量组：查询组、键组和值组。 在每组中，有 h 个向量，维度为 $d_{q’} = d_{k’} = d_{v’} = d_{model}/h = 64$。然后将来自不同输入的向量打包成三组不同的矩阵：${Q_i}^h_i=1, {K_i}h_{i=1}$ 和${V_i}^h_i=1$.多头注意力程序如下所示：<br>$$<br>MultiHead(Q′, K′, V′) = Concat(head1, · · · , headh)Wo, where headi = Attention(Q_i, K_i, V_i).<br>$$<br>此处$Q’$（以及类似的$ K’ $和 $V’$）是 ${Qi}^h_i=1 $的串联，$W^o ∈ R^d_{model}×d_{model} $是线性投影矩阵。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214161941.png" style="zoom:50%"><center>图 4：Transformer的结构。图像来自[225]。</center><h3 id="2-3-Transformer中的其他关键概念"><a href="#2-3-Transformer中的其他关键概念" class="headerlink" title="2.3 Transformer中的其他关键概念"></a>2.3 Transformer中的其他关键概念</h3><p><strong>编码器和解码器中的剩余连接</strong>。如图4所示，将一个残余连接添加到编码器和解码器的每个子层。这加强了信息的流动，以实现更高的性能。在剩余连接之后是层规范化[5]。这些操作的输出可以描述为:<br>$$<br>LayerNorm(X + Attention(X)).\tag{11}<br>$$<br>此处，$X$作为自我注意层的输入。这是因为查询矩阵$Q$，键矩阵$K$，值矩阵$V$都是从同一个输入矩阵$X$​派生出来的。对于归一化层，有几种替代方法，例如批量归一化 [107]。 由于特征值急剧变化[198]，批量归一化在应用于transformer时通常表现更差。 已经提出了一些其他归一化算法 [249]、[198]、[5] 来改进变压器的训练。</p><p>**应用前馈网络(FFN)**。在每个编码器和解码器中的自注意力层之后应用前馈网络 (FFN)。 它由两个线性变换层和其中的非线性激活函数组成，可以表示为以下函数：<br>$$<br>FFN ⁡ ( X ) = W_{2σ} ( W_1 X ) \tag{12}<br>$$<br>其中$W_1$ 和$W_2$为两个线性变换层的两个参数矩阵，$σ$为非线性激活函数，如GELU[91]。隐含层的维数$d_h = 2048 $。</p><p><strong>解码器的最后一层</strong>。解码器的最后一层用于将向量堆栈转换回一个单词。这是通过一个线性层和一个softmax层来实现的。线性层将向量投影到一个具有$d_{word}$ 维数的逻辑向量中，其中$d_{word}$ 是词汇表中的单词数。softmax层然后被用来把逻辑向量转换成概率。</p><p>​ 用于CV任务时，大多数Transformer采用原Transformer的编码器模块。这样的Transformer可以看作是一种新的特征选择器。与只关注局部特征的CNN相比，Transformer可以捕捉到长距离特征，这意味着它可以很容易地获得全局信息。与必须连续计算隐藏状态的RNNs相比，Transformer的效率更高，因为自注意层和全连接层的输出可以并行计算，并且易于加速。由此我们可以得出结论，在计算机视觉和自然语言处理中进一步研究变压器将会取得有益的结果。</p><h2 id="3-重温-Transformers应用于NLP"><a href="#3-重温-Transformers应用于NLP" class="headerlink" title="3. 重温 Transformers应用于NLP"></a>3. 重温 Transformers应用于NLP</h2><p>​ 在transformer被开发出来之前，添加attention的RNNs（如GRU[47]和LSTM[94]）属于最先进的语言模型。然而，RNNs要求信息流从前一个隐藏状态依次处理到下一个隐藏状态。这就排除了在训练过程中使用加速和并行化的可能性，从而阻碍了RNN处理更长的序列或构建更大模型的潜力。2017年，Vaswani等人[225]提出了变压器，这是一种新的编码器-解码器结构，仅建立在多头自注意机制和前馈神经网络上。它的目的是通过获取全局依赖项来轻松解决seq-to-seq自然语言任务（如机器翻译）。Transformer随后的成功证明，单独利用注意机制可以获得与注意神经网络相当的性能。此外，transformer的架构适合大规模并行计算，这使得在更大的数据集上进行训练成为可能。这导致了用于自然语言处理的大型预训练模型（PTMs pre-trained models ）的激增。</p><p>​ BERT[50]及其变体（如SpanBERT[115]、RoBERTa[146]）是建立在多层变压器编码器架构上的一系列PTMs。在BERT的预训练阶段，对图书语料库[292]和英文维基百科数据集进行了两个任务：1. 屏蔽语言建模(MLM Masked language modeling)，首先随机屏蔽输入中的一些标记，然后训练模型进行预测；2. 下一句预测，使用成对的句子作为输入，预测第二句是否是文档中的原句。在预培训之后，可以通过在广泛的下游任务上添加输出层来对BERT进行微调。更具体地说，在执行序列级任务(如情感分析)时，BERT使用第一个表征的表示进行分类；对于表征级任务（例如，名称实体识别），所有表征都被送入softmax层进行分类。在它发布的时候，BERT在11个NLP任务中取得了最先进的性能，在预先训练的语言模型中树立了一个里程碑。预先训练的生成式变压器模型（例如:GPT[181]、GPT-2[182]是另一种基于变压器译码架构的 PTMs，它使用了掩藏的自我注意机制。GPT系列和BERT之间的主要区别在于进行预培训的方式。与BERT不同，GPT模型是使用左-右(LTR Left-to-Right ) 语言建模预先训练的单向语言模型。此外，BERT在预训练期间学习了句子分隔符（[SEP] separator）和分类标记（[CLS] classifier token）嵌入，而这些嵌入仅涉及GPT的微调阶段。由于它的单向预训练策略，GPT在许多自然语言生成任务中取得了优越的性能。最近，一个名为GPT-3的巨型transformer模型被开发出来，它有惊人的1750亿个参数。GPT-3通过对45tb的压缩明文数据进行预训练，可以直接处理不同类型的下游自然语言任务，无需进行微调。结果表明，该算法在许多NLP数据集上都取得了良好的性能，包括自然语言的理解和生成。自从变压器的引入，除了前面提到的基于变压器的PTMs之外，还有许多其他的模型被提出。为了方便感兴趣的读者，我们在表2中列出了一些有代表性的模型，但这不是我们研究的重点。</p><center>表2:基于transformer的代表性语言模型列表。Transformer是标准的编码器-解码器架构。其中，变压器encl和dec分别表示编码器和解码器。解码器使用掩码自我注意来防止注意到未来的令牌。该表的数据来自[179]。</center><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214164210.png" style="zoom:67%"><p>除了在大型语料库上训练用于一般自然语言处理任务的PTMs外，基于transformer的模型还被应用于许多其他与自然语言处理相关的领域和多模态任务。</p><p><strong>BioNLP 领域</strong>。 基于transformer的模型优于许多传统的生物医学方法。这类模型的一些例子包括BioBERT[123]，它使用transformer架构进行生物医学文本挖掘任务，以及SciBERT[11]，它是通过对1.14亿篇科学文章(涵盖生物医学和计算机科学领域)训练transformer而开发的，目的是更精确地在科学领域执行NLP任务。另一个例子是Huang等人提出的ClinicalBERT[101]。它利用Transformer来开发和评估临床记录的连续表示。这样做的另外作用是，ClinicalBERT的attention map可以用来解释预测，从而允许不同医学内容之间的高质量联系被发现。</p><p>​ 基于 Transformer 的模型在各种 NLP 相关任务上的快速发展展示了其结构优势和通用性，开启了它成为应用于 NLP 以外的许多 AI 领域的通用模块的可能性。 本次调查的以下部分重点介绍了 Transformer 在过去两年出现的各种计算机视觉任务中的应用。</p><h1 id="4-视觉-Transformer"><a href="#4-视觉-Transformer" class="headerlink" title="4. 视觉 Transformer"></a>4. 视觉 Transformer</h1><p>在本节中，我们将回顾基于Transformer的模型在计算机视觉中的应用，包括图像分类、高/中级视觉、低级视觉和视频处理。简要总结了自注意机制和模型压缩方法在高效Transformer中的应用。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214164649.png" style="zoom:67%"><center>图5: 使用卷积和注意力进行表征学习的主干分类法。</center><h2 id="4-1-表征学习的主干"><a href="#4-1-表征学习的主干" class="headerlink" title="4.1 表征学习的主干"></a>4.1 表征学习的主干</h2><p>受Transformer在自然语言处理领域取得成功的启发，一些研究人员开始探索类似的模型是否可以学习到图像的有用表示。由于与文本相比，图像涉及更多的尺寸、噪声和冗余模态，因此它们被认为更难以生成建模。</p><p>​ 除了 CNNs，transformer 可以用作主干用于图像分类的网络。吴等人[<a target="_blank" rel="noopener" href="https://translate.googleusercontent.com/translate_f#24">240</a> ] 采用了 ResNet作为方便的基线并使用视觉转换器来替换卷积的最后阶段。具体来说，他们应用卷积层提取低级特征，然后输入视觉Transformer。对于视觉转换器，他们使用标记器将像素分组为少量的视觉标记，每个代表图像中的语义概念。这些视觉令牌直接用于图像分类，与前者用于对令牌之间的关系进行建模。作为</p><p>如图5所示，作品可以分为纯粹使用视觉的Transformer，结合 CNN的Transformer。我们在表3和图 7-8 中总结了这些模型的结果</p><p>展示骨干的发展。此外监督学习，自监督学习也在探索视觉Transformer。</p><center>表 3：代表性 CNN 和视觉变换器模型的 ImageNet 结果比较。 在 [219]、[148] 之后，吞吐量是在 NVIDIA V100 GPU 和 Pytorch 上测量的，输入大小为 224×224。 纯transformer意味着在stem阶段只使用几个卷积。 CNN + Transformer 意味着在中间层使用卷积。</center><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214171936.png" style="zoom:50%"><h4 id="纯Transformer"><a href="#纯Transformer" class="headerlink" title="纯Transformer"></a>纯Transformer</h4><p><strong>VIT</strong>。Dosovitskiy等 [55]最近提出了视觉Transformer（ViT），它是一种纯Transformer，当直接应用于图像块序列时，它在图像分类任务上表现良好。 他们尽可能地遵循变压器的原始设计。 图 6 显示了 ViT 的框架。</p><p>为了处理 2D 图像，图像$ X ∈ R^{h×w×c} $被重新塑造成一系列扁平的 2D 块 $X_p ∈ R^{n×(p2·c)}$，其中 c 是通道数。 (h, w) 是原始图像的分辨率，而 (p, p) 是每个图像块的分辨率。 因此，变压器的有效序列长度为$ n = hw/p^2$​。 由于转换器在其所有层中使用恒定宽度，因此可训练的线性投影将每个矢量化路径映射到模型维度 d，其输出称为补丁嵌入。</p><p>​ 与 BERT 的 [class] 标记类似，可学习的嵌入应用于嵌入补丁序列。 这种嵌入的状态用作图像表示。 在预训练和微调阶段，分类头连接到相同的大小。 此外，将一维位置嵌入添加到补丁嵌入中以保留位置信息。 Dosovitskiy等已经探索了位置嵌入的不同 2D 感知变体，但没有一个比标准的 1D 位置嵌入产生任何显着的收益。 联合嵌入作为编码器的输入。 值得注意的是，ViT 仅使用标准变压器的编码器（除了用于层归一化），其输出在 MLP 头之前。</p><p>​ 在大多数情况下，ViT 在大型数据集上进行了预训练，然后针对较小的下游任务进行了微调。 为此，去除了预训练的预测头，并附加了一个零初始化的 d × k 前馈层，其中 k 是下游类的数量。 在微调阶段使用比预训练阶段更高的分辨率通常是有益的。 例如，当输入更高分辨率的图像时可以获得更大的有效序列长度，即使补丁大小保持不变。 尽管 ViT 可以处理任意序列长度，但预训练的位置嵌入可能不再有意义。 多索维茨基等。 因此，根据它们在原始图像中的位置对预训练的位置嵌入进行二维插值。 请注意，只有在分辨率调整和补丁提取期间，才会将有关图像 2D 结构的归纳偏差手动注入 ViT。</p><p>​ 在中等规模的数据集（如 ImageNet）上训练时，ViT 会产生适度的结果，实现的准确度比同等规模的 ResNet 低几个百分点。 由于 Transformer 缺乏 CNN 固有的一些归纳偏差——例如平移等方差和局部性——当在数据量不足的情况下进行训练时，它们不能很好地泛化。 然而，作者发现在大型数据集（1400 万到 3 亿张图像）上训练模型超过了归纳偏差。 当以足够的规模进行预训练时，transformer 在数据点较少的任务上取得了出色的结果。 例如，当在 JFT-300M 数据集上进行预训练时，ViT 在多个图像识别基准上接近甚至超过了最先进的性能。 具体来说，它在 ImageNet 上的准确率达到了 88.36%，在 CIFAR-10 上达到了 99.50%，在 CIFAR-100 上达到了 94.55%，在 VTAB 套件的 19 个任务上达到了 77.16%。</p><p>​ Touvron等人 [219] 通过仅在 ImageNet 数据库上进行训练，提出了一种竞争性的无卷积变换器，称为数据高效图像变换器（DeiT）。 DeiT-B 是参考视觉转换器，与 ViT-B 具有相同的架构，并采用了 8600 万个参数。 通过强大的数据增强，DeiT-B 在没有外部数据的情况下在 ImageNet 上达到了 83.1%（单作物评估）的 top-1 准确率。 此外，作者观察到使用 CNN 教师比使用变压器具有更好的性能。 具体来说，DeiT-B 在基于代币的蒸馏的帮助下可以达到 84.40% 的 top-1 准确率。</p><p><strong>Variants of ViT</strong>。 遵循 ViT 的范式，已经提出了一系列 ViT 的变体来提高视觉任务的性能。 主要方法包括增强局部性、自注意力改进和架构设计。</p><p>​ 原始视觉变换器擅长捕捉补丁之间的长距离依赖关系，但忽略了局部特征提取，因为 2D 补丁被投影到具有简单线性层的向量。 最近，研究人员开始关注提高对局部信息的建模能力[85]、[148]、[26]。 TNT [85] 进一步将补丁划分为多个子补丁，并引入了一种新颖的转换器中转换器架构，该架构利用内部转换器块来模拟子补丁和外部转换器块之间的关系，以进行补丁级别的信息交换 . Twins [43] 和 CAT [137] 交替地逐层执行局部和全局注意。 Swin Transformers [148], [54] 在窗口内执行局部注意力，并为跨窗口连接引入了一种移位窗口分区方法。 Shuffle Transformer [105], [63] 进一步利用空间混洗操作而不是移位窗口分区来允许跨窗口连接。 RegionViT [26] 从图像中生成区域标记和本地标记，本地标记通过区域标记的注意力接收全局信息。 除了局部注意力之外，其他一些工作还提出通过局部特征聚合来增强局部信息，例如 T2T [260]。 这些工作证明了视觉转换器中本地信息交换和全局信息交换的好处。</p><p>​ 作为transformer的关键组件，自注意力层提供了图像块之间全局交互的能力。 改进self-attention layer的计算已经吸引了很多研究人员。 DeepViT [286] 建议建立交叉头通信以重新生成注意力图以增加不同层的多样性。 KVT [230] 引入了 k-NN 注意力以利用图像块的局部性并通过仅计算具有前 k 个相似标记的注意力来忽略嘈杂的标记。 Refiner [287] 探索了高维空间中的注意力扩展，并应用卷积来增强注意力图的局部模式。 XCiT [56] 跨特征通道而不是标记执行自注意力计算，这允许高效处理高分辨率图像。 自注意力机制的计算复杂度和注意力精度是未来优化的两个关键点。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214171751.png" style="zoom:67%"><p>​ 正如 CNN 领域所证明的那样，网络架构是重要的因素。 ViT 的原始架构是相同形状的 Transformer 块的简单堆栈。 视觉转换器的新架构设计一直是一个有趣的话题。 许多视觉转换器模型 [232]、[148]、[209]、[61]、[279]、[167] 都使用了金字塔状架构，包括 PVT [232]、HVT [168]、Swin Transformer [148] 和 PiT [92]。 还有其他类型的架构，例如双流架构[25]和U-net架构[237]、[17]。 还研究了神经架构搜索 (NAS) 以搜索更好的变压器架构，例如 Scaling-ViT [269]、ViTAS [205]、AutoFormer [28] 和 GLiT [24]。 目前，视觉变换器的网络设计和 NAS 都主要借鉴 CNN 的经验。 未来，我们期待在视觉转换器领域出现特定的、新颖的架构。</p><p>​ 除了上述方法之外，还有一些其他方向可以进一步改进视觉变换器，例如位置编码[44]、[242]、归一化策略[220]、快捷连接[215]和去除注意力[217]， [158]、[79]、[218]。</p><h4 id="4-1-2-Transformer-与卷积"><a href="#4-1-2-Transformer-与卷积" class="headerlink" title="4.1.2 Transformer 与卷积"></a>4.1.2 Transformer 与卷积</h4><p>​ 尽管视觉转换器由于能够捕获输入中的长距离依赖关系而已成功应用于各种视觉任务，但Transformer与现有 CNN 之间的性能仍然存在差距。 一个主要原因可能是缺乏提取本地信息的能力。 除了上面提到的增强局部性的 ViT 变体之外，将Transformer与卷积相结合可以是将局部性引入传统Transformer的更直接的方法。 有很多工作试图用卷积来增强传统的Transformer块或自注意力层。 例如，CPVT [44] 提出了一种条件位置编码 (CPE) 方案，该方案以输入标记的局部邻域为条件并适应任意输入大小，以利用卷积进行精细级别的特征编码。 CvT [241]、CeiT [259]、LocalViT [132] 和 CMT [77] 分析了直接从 NLP 借用 Transformer 架构并将卷积与 Transformer 结合在一起时的潜在缺点。 具体来说，每个Transformer块中的前馈网络 (FFN) 与一个卷积层相结合，以促进相邻令牌之间的相关性。 LeViT [75] 重新审视了大量关于 CNN 的文献中的原理，并将它们应用于Transformer，提出了一种用于快速推理图像分类的混合神经网络。 BoTNet [202] 在 ResNet 的最后三个瓶颈块中用全局自注意力替换了空间卷积，并在实例分割和对象检测任务上显着改进了基线，延迟开销最小。 此外，一些研究人员已经证明，基于变换器的模型可能更难以享受良好的拟合数据的能力 [55]、[38]、[245]，换句话说，它们对优化器、超参数的选择很敏感 ，以及训练时间表。 Visformer [38] 揭示了具有两种不同训练设置的Transformer和 CNN 之间的差距。 第一个是 CNN 的标准设置，即训练时间较短，数据增强仅包含随机裁剪和水平翻转。 另一个是[219]中使用的训练设置，即训练时间更长，数据增强更强。 [245] 通过用标准卷积茎替换其嵌入茎来改变 ViT 的早期视觉处理，并发现这种变化使 ViT 能够更快地收敛并能够使用 AdamW 或 SGD，而不会显着降低准确性。 除了这两个作品，[75]、[77]还选择在Transformer的顶部添加卷积。</p><h4 id="4-1-3-自监督表征学习"><a href="#4-1-3-自监督表征学习" class="headerlink" title="4.1.3 自监督表征学习"></a>4.1.3 自监督表征学习</h4><p><strong>基于生成的方法</strong>。 图像的生成式预训练方法已经存在很长时间了。 陈等人 [29] 重新检查了此类方法并将其与自监督方法相结合。 之后，提出了几项工作 [134]、[8] 来扩展基于生成的自监督学习的视觉变换器。 我们简要介绍 iGPT [29] 以演示其机制。 这种方法包括一个预训练阶段，然后是一个微调阶段。 在预训练阶段，探索了自回归和 BERT 目标。 为了实现像素预测，采用了序列转换器架构而不是语言标记（如在 NLP 中使用的）。 当与提前停止结合使用时，预训练可以被认为是一种有利的初始化或正则化器。 在微调阶段，他们向模型添加了一个小的分类头。 这有助于优化分类目标并适应所有权重。</p><p>​ 通过k-means聚类将图像像素转换为序列数据。 给定一个由高维数据 $x = (x_1 , · · · , x_n ) $组成的未标记数据集 X，他们通过最小化数据的负对数似然来训练模型：<br>$$<br>LAR = E_{x~X} [−logp(x)] \tag{13}<br>$$<br>其中 p(x) 是图像数据的概率密度，可以建模为：<br>$$<br>p(x) = \prod_{i=1}^np(x_{π_i}|x_{π_1},··· ,x_{π_{i−1}},θ). \tag{14}<br>$$<br><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214173213.png" style="zoom:50%"></p><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p>视觉转换器的所有组件，包括多头自注意力、多层感知器、快捷连接、层归一化、位置编码和网络拓扑，在视觉识别中起着关键作用。 如上所述，已经提出了许多工作来提高视觉变换器的有效性和效率。 从图 7-8 的结果中，我们可以看到 CNN 和 Transformer 的结合获得了更好的性能，表明它们通过局部连接和全局连接相互补充。 对骨干网络的进一步调查可以导致整个视觉社区的改进。 至于视觉变换器的自监督表示学习，我们还需要努力追求NLP领域大规模预训练的成功。</p><h3 id="4-2-高-中层视觉"><a href="#4-2-高-中层视觉" class="headerlink" title="4.2 高/中层视觉"></a>4.2 高/中层视觉</h3><p>最近，人们对使用 Transformer 进行中高级计算机视觉任务越来越感兴趣，例如目标检测 [19]、[291]、[10]、[263]、[166]、车道检测 [144] 、分割[235]、[228]、[285]和姿态估计[102]、[103]、[138]、[253]。 我们将在本节中回顾这些方法。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214174618.png" style="zoom:67%"><center>图 9：基于变压器的目标检测的一般框架。 (a) 对象检测任务通过变换器（例如，DETR [19]）被制定为集合预测任务。 (b) 使用视觉转换器代替传统物体检测器的 CNN 主干（例如 ViT-FRCNN [10]）。</center><h4 id="4-2-1-常规目标检测"><a href="#4-2-1-常规目标检测" class="headerlink" title="4.2.1 常规目标检测"></a>4.2.1 常规目标检测</h4><p>​ 传统的物体检测器主要建立在 CNN 上，但基于变压器的物体检测由于其优越的能力，最近引起了人们的极大兴趣。</p><p>​ 一些物体检测方法尝试使用Transformer的自注意力机制，然后增强现代检测器的特定模块，例如特征融合模块 [271] 和预测头 [41]。 我们稍后将在第 5 节讨论这一点。基于 Transformer 的对象检测方法大致分为两类：基于 Transformer 的集合预测方法 [19]、[291]、[210]、[284]、[154] 和基于Transformer的主干方法 [10]、[166]，如图 9 所示。与基于 CNN 的检测器相比，基于Transformer的方法在准确性和运行速度方面都表现出强大的性能。 表 4 显示了前面提到的 COCO 2012 val 集上不同的基于Transformer的物体检测器的检测结果。</p><p>​ 用于检测的基于Transformer的集合预测。 作为基于Transformer的检测方法的先驱，Carion 等人提出的检测Transformer（DETR） [19]重新设计了目标检测的框架。 DETR 是一种简单且完全端到端的对象检测器，将对象检测任务视为直观的集合预测问题，消除了传统的手工组件，例如锚点生成和非最大抑制 (NMS) 后处理。 如图 10 所示，DETR 从 CNN 主干开始，从输入图像中提取特征。 为了用位置信息补充图像特征，在特征被送入编码器-解码器转换器之前，将固定位置编码添加到扁平特征中。 解码器使用来自编码器的嵌入以及 N 个学习的位置编码（对象查询），并产生 N 个输出嵌入。 这里 N 是预定义的参数，通常大于图像中的对象数量。 简单的前馈网络 (FFN) 用于计算最终预测，其中包括边界框坐标和类标签以指示特定的对象类（或指示不存在对象）。 与按顺序计算预测的原始转换器不同，DETR 并行解码 N 个对象。 DETR 采用二分匹配算法来分配预测对象和真实对象。 如方程式所示。 匈牙利损失被用来计算所有匹配的对象对的损失函数。</p><p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214174128.png"></p><p>​ 其中σˆ是最优分配，$ci$和$pˆσˆ(i)(ci)$分别是目标类标签和预测标签，而$bi$和$ˆbσˆ(i)$分别是真实情况和预测的边界框，$y = {(ci,bi)} $和$ yˆ $分别是对象的真实情况和预测。 DETR 在对象检测方面表现出令人印象深刻的性能，在 COCO 基准上提供与流行且完善的 Faster R-CNN [186] 基线相当的准确性和速度。</p><p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214174304.png"></p><center>图 10：DETR 的整体架构。 图像来自 [19]。</center><p>​ DETR 是基于 Transformer 的对象检测框架的新设计，并赋予社区开发完全端到端的检测器的能力。 然而，vanilla DETR 带来了一些挑战，特别是较长的训练时间表和小物体的性能不佳。 为了应对这些挑战，朱等人 [291] 提出了 Deformable DETR，它已成为一种流行的方法，显着提高了检测性能。 可变形注意力模块关注参考点周围的一小组关键位置，而不是像 Transformer 中原始的多头注意力机制那样查看图像特征图上的所有空间位置。 这种方法显着降低了计算复杂度，并带来了快速收敛的好处。 更重要的是，可变形注意力模块可以轻松应用于融合多尺度特征。 Deformable DETR 的性能比 DETR 更好，训练成本降低 10 倍，推理速度提高 1.6 倍。 并且通过使用迭代边界框细化方法和两阶段方案，Deformable DETR 可以进一步提高检测性能。</p><p>​ 还有几种方法可以处理原始DETR的收敛速度慢的问题。 例如，Sun 等人 [210] 调查了 DETR 模型收敛缓慢的原因，发现这主要是由于 Transformer 解码器中的交叉注意模块。 为了解决这个问题，提出了仅编码器版本的 DETR，在检测精度和训练收敛方面取得了相当大的改进。 此外，设计了一种新的二分匹配方案以提高训练稳定性和更快收敛速度，并提出了两个基于变换器的集预测模型，即 TSP-FCOS 和 TSP-RCNN，以改进具有特征金字塔的仅编码器 DETR。 与原始 DETR 模型相比，这些新模型实现了更好的性能。 高等人 [71] 提出了空间调制共同注意（SMCA）机制，通过将共同注意响应限制在初始估计的边界框位置附近的高值来加速收敛。 通过将所提出的 SMCA 模块集成到 DETR 中，可以在可比较的推理成本下以大约 10 倍的训练周期获得类似的 mAP。</p><p>​ 鉴于与 DETR 相关的高计算复杂性，Zheng 等人[284] 提出了一种自适应聚类Transformer（ACT）来降低预训练 DETR 的计算成本。 ACT 使用局部敏感性散列 (LSH) 方法自适应地对查询特征进行聚类，并将注意力输出广播到由所选原型表示的查询。 ACT 用于替换预训练的 DETR 模型的自注意力模块，无需任何重新训练。 这种方法显着降低了计算成本，同时精度略有下降。 通过利用多任务知识蒸馏 (MTKD) 方法可以进一步减少性能下降，该方法利用原始转换器通过几个时期的微调来提取 ACT 模块。 姚等人 [257]指出DETR中的随机初始化是需要多个解码器层和缓慢收敛的主要原因。 为此，他们提出了 Efficient DETR，通过额外的区域提议网络将密集先验合并到检测管道中。 更好的初始化使他们能够仅使用一个解码器层而不是六层，以通过更紧凑的网络实现具有竞争力的性能。</p><p><strong>用于检测的基于变压器的骨干网</strong>。 与 DETR 不同，DETR 通过 Transformer 将目标检测重新设计为一组预测任务，Beal 等人 [10] 提议利用 Transformer 作为常见检测框架的主干，例如 Faster R-CNN [186]。 输入图像被分成若干块并输入到视觉变换器中，其输出嵌入特征在通过检测头获得最终结果之前根据空间信息进行重组。 大规模的预训练Transformer主干可以为提议的 ViT-FRCNN 带来好处。 还有很多方法可以探索多功能视觉转换器主干设计 [85]、[232]、[148]、[43] 并将这些主干转移到传统检测框架，如 RetinaNet [140] 和 Cascade R-CNN [ 16]。 例如，Swin Transformer [148] 在 ResNet-50 主干上获得了大约 4 个框 AP 增益，具有类似的 FLOP，用于各种检测框架。</p><p><strong>用于医学图像分割的Transformer</strong>。 曹等人 [17] 提出了一种用于医学图像分割的类似 Unet 的纯 Transformer，通过将标记化的图像块输入到基于 Transformer 的 U 形编码器 - 解码器架构中，并具有用于局部全局语义特征学习的跳过连接。 瓦拉那拉苏等人 [223] 探索了基于变压器的解决方案并研究了使用基于Transformer的网络架构进行医学图像分割任务的可行性，并提出了一种门控轴向注意模型，该模型通过在自注意力模块中引入额外的控制机制来扩展现有架构。 Cell-DETR [174] 基于 DETR 全景分割模型，尝试使用 Transformer 进行单元实例分割。 它在分割头中添加了桥接主干 CNN 和 CNN 解码器之间的特征的跳过连接，以增强特征融合。 Cell-DETR 实现了从显微镜图像中分割细胞实例的最先进性能。</p><h4 id="姿态估计"><a href="#姿态估计" class="headerlink" title="姿态估计"></a>姿态估计</h4><p><strong>人体姿势和手部姿势估计是引起研究界极大兴趣的基础主题</strong>。 关节姿态估计类似于结构化预测任务，旨在从输入的 RGB/D 图像中预测关节坐标或网格顶点。 在这里，我们讨论了一些方法 [102]、[103]、[138]、[253]，这些方法探索了如何利用 Transformer 对人体姿势和手部姿势的全局结构信息进行建模。 用于手姿势估计的变压器。 黄等人 [102] 提出了一种基于变换器的网络，用于从点集进行 3D 手部姿势估计。 编码器首先利用 PointNet [177] 从输入点云中提取逐点特征，然后采用标准的多头自注意力模块生成嵌入。 为了向解码器公开更多与全局姿态相关的信息，使用诸如 PointNet++ [178] 之类的特征提取器来提取手部关节特征，然后将其作为位置编码输入解码器。 同样，黄等人 [103] 提出了用于 3D 手部对象姿态估计的 HOT-Net（手部对象Transformer网络的简称）。 与前面使用变换器直接从输入点云预测 3D 手部姿态的方法不同，HOT-Net 使用 ResNet 生成初始 2D 手部对象姿态，然后将其输入变换器来预测 3D 手部对象姿态。 因此，谱图卷积网络用于提取编码器的输入嵌入。 汉帕利等人 [81] 建议在给定单色图像的情况下估计两只手的 3D 姿势。 具体来说，将双手关节的一组潜在 2D 位置的外观和空间编码输入到转换器中，并使用注意力机制来挑选关节的正确配置并输出双手的 3D 姿势。</p><p><strong>人体姿势估计变压器</strong>。 林等人 [138] 提出了一种网格Transformer (METRO)，用于从单个 RGB 图像预测 3D 人体姿势和网格。 METRO 通过 CNN 提取图像特征，然后通过将模板人体网格连接到图像特征来执行位置编码。 提出了一种具有渐进降维的多层变压器编码器，以逐渐减少嵌入维度，最终生成人体关节和网格顶点的 3D 坐标。 为了鼓励学习人体关节之间的非局部关系，METRO 在训练期间随机屏蔽了一些输入查询。 杨等人 [253] 基于 Transformer 架构和低级卷积块构建了一个名为 TransPose 的可解释模型。 Transformer 中内置的注意力层可以捕获关键点之间的长距离空间关系，并解释预测的关键点位置高度依赖的依赖关系。 李等人。 [133] 提出了一种基于令牌表示的人体姿势估计（TokenPose）的新方法。 每个关键点都被明确嵌入为令牌，以同时从图像中学习约束关系和外观线索。 毛等人 [156] 提出了一种人体姿势估计框架，该框架以基于回归的方式解决了该任务。 他们将姿势估计任务制定为序列预测问题，并通过转换器解决该问题，从而绕过了基于热图的姿势估计器的缺点。 江等人[110] 提出了一种新的基于变换器的网络，它可以以无监督的方式学习姿势和运动的分布，而不是跟踪身体部位并尝试在时间上平滑它们。 该方法克服了检测的不准确性并纠正了部分或整个骨架损坏。 郝等人[86] 建议在不使用任何手动注释的情况下，在给定一组人的测试图像的情况下个性化人体姿势估计器。 该方法在测试期间调整姿势估计器以利用特定于个人的信息，并使用 Transformer 模型在自监督关键点和监督关键点之间建立转换。</p><h3 id="4-2-5-讨论"><a href="#4-2-5-讨论" class="headerlink" title="4.2.5 讨论"></a>4.2.5 讨论</h3><p>​ 正如前面几节所讨论的，transformer 在几个高级任务上表现出强大的性能，包括检测、分割和姿态估计。 在高级任务中采用 Transformer 之前需要解决的关键问题涉及输入嵌入、位置编码和预测损失。 一些方法建议从不同的角度改进自注意力模块，例如可变形注意力 [291]、自适应聚类 [284] 和点变换器 [280]。 尽管如此，将转换器用于高级视觉任务的探索仍处于初步阶段，因此进一步的研究可能会证明是有益的。 例如，是否有必要在 Transformer 之前使用 CNN 和 PointNet 等特征提取模块以获得更好的性能？ 如何像 BERT 和 GPT-3 在 NLP 领域那样使用大规模预训练数据集充分利用视觉转换器？ 是否有可能预训练单个 Transformer 模型并针对不同的下游任务对其进行微调，而只需要几个时期的微调？ 如何通过结合特定任务的先验知识来设计更强大的架构？ 之前的一些工作已经对上述主题进行了初步讨论，我们希望进行更多的研究工作，探索更强大的用于高级视觉的转换器。</p><h3 id="4-3-Low-level-Vision"><a href="#4-3-Low-level-Vision" class="headerlink" title="4.3 Low-level Vision"></a>4.3 Low-level Vision</h3><p>​ 很少有研究将Transformer应用于低视场，如图像的超分辨率和生成。这些任务通常以图像作为输出(例如，高分辨率或去噪图像)，这比高级视觉任务更具挑战性，如分类、分割和检测，这些任务的输出是标签或框。</p><h4 id="4-3-1-图片生成"><a href="#4-3-1-图片生成" class="headerlink" title="4.3.1 图片生成"></a>4.3.1 图片生成</h4><p>​ Parmar等[171]提出了Image Transformer，第一步是将Transformer模型推广到制定图像转换和生成任务。图像Transformer由两部分组成:用于提取图像表示的编码器和用于生成像素的解码器。对于值为0到255的每个像素，学习256 × d 256 × d256×d维嵌入，将每个值编码为d维向量，并将其作为输入输入编码器。编码器和解码器采用与[152]相同的架构。图8显示了在译码器每个层的结构。通过计算输入像素，对于图像条件生成，如超分辨率和修复，使用编码器-解码器架构，其中编码器的输入是低分辨率或损坏的图像。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214175353.png" style="zoom:67%"><center>图 11：Taming Transformer 架构图（图来自 [58]）。 卷积 VQGAN 用于学习码本，而Transformer用于学习视觉代码。</center><p>​ 对于无条件和类条件生成(即噪声到图像)，只有解码器用于输入噪声向量。由于解码器的输入是之前生成的像素(在生成高分辨率图像时涉及到较高的计算成本)，因此提出了一种局部自注意方案。该方案仅使用最接近的生成像素作为解码器的输入，使图像转换器在图像生成和翻译任务中达到与基于cnn的模型同等的性能，证明了基于转换器的模型在低层次视觉任务中的有效性。</p><p>​ 最近的一些作品避免使用每个像素作为Transformer模型的输入，而是使用补丁(像素集)作为输入。例如，yang等[167]提出了用于图像超分辨率的纹理Transformer网络(Texture Transformer Network For Image superresolution, TTSR)，利用了基于参考的图像超分辨率问题中的Transformer架构。它的目的是将相关的纹理从参考图像转移到低分辨率的图像。以低分辨率图像和参考图像分别为查询Q和键K，提出了一种硬注意模块，根据参考图像选择高分辨率特征V，利用相关性对低分辨率图像进行匹配，和F分别为低分辨率图像的输出特征和输入特征;S SS是柔和的注意;T TT为从高分辨率纹理图像中转移出来的特征。通过利用基于Transformer 的架构，TTSR能够在超分辨率任务中成功地将纹理信息从高分辨率参考图像转移到低分辨率图像。</p><p>​ 不同于之前在单个任务中使用Transformer 模型的方法，Chen等人[20]提出了图像处理Transformer(Image Processing transformer, IPT)，它利用了大量的预训练数据集，充分利用了变压器的优点。它在一些图像处理任务中实现了最先进的性能，包括超分辨率、去噪和去雨。</p><p>​ 如图9所示，IPT包括多头、一个编码器、一个解码器和多尾组成。针对不同的图像处理任务，引入了多头、多尾结构和任务嵌入。特征被划分成小块，并被输入到编码器-解码器体系结构中。随后，输出被重塑为具有相同大小的特征。鉴于在大数据集上预训练变压器模型的优点，IPT使用ImageNet数据集进行预训练。具体来说，该数据集中的图像通过手动添加噪声、雨条纹或降采样来退化，从而生成损坏的图像。以退化后的图像作为IPT的输入，以原始图像作为输出的优化目标。为了提高模型的泛化能力，还引入了一种自监督方法。一旦模型得到训练，它就会通过使用相应的头、尾和任务嵌入对每个任务进行微调。IPT在很大程度上提高了图像处理任务的性能(如图像去噪任务的性能为2 dB)，显示了基于变压器的模型在低层次视觉领域的巨大潜力。</p><p>​ 除单图像生成外，Wang等[158]还提出了Scene Former，利用Transformer进行室内三维场景生成。通过将场景视为一系列物体，transformer解码器可以用来预测一系列物体及其位置、类别和大小。这使得SceneFormer在用户研究中优于传统的基于cnn的方法。</p><p>​ 综上所述，与分类和检测任务不同，图像生成和处理的输出是图像。图10演示了在低级视图中使用变压器。通过将图像作为像素序列或小块，transformer编码器使用该序列作为输入，允许transformer解码器成功地生成所需的图像。为不同的图像处理任务设计一个合适的体系结构是未来研究的一个有意义的方向。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214175616.png" style="zoom:67%"><center>图 12：IPT 架构图（图来自 [27]）。 多头多尾结构处理不同的图像处理任务。</center><h3 id="4-6-高效-Transformer"><a href="#4-6-高效-Transformer" class="headerlink" title="4.6. 高效 Transformer"></a>4.6. 高效 Transformer</h3><pre><code>     虽然Transformer模型在各种任务中都取得了成功，但它们对内存和计算资源的高要求阻碍了它们在资源有限的设备(如手机)上的实现。在本节中，我们回顾了为了有效实施而压缩和加速变压器模型的研究。这包括网络剪枝、低秩分解、知识精馏、网络量化和紧凑架构设计。表5列出了基于变压器的模型压缩的一些代表性工作。
</code></pre><h4 id="4-6-1-Pruning-and-Decomposition"><a href="#4-6-1-Pruning-and-Decomposition" class="headerlink" title="4.6.1 Pruning and Decomposition"></a>4.6.1 Pruning and Decomposition</h4><p>​ 在基于transformer的预训练模型(如BERT)中，多重注意操作是并行进行的，以独立建模不同表征之间的关系[152,34]。然而，特定任务并不要求使用所有的磁头。</p><p>​ 例如，Michel等人[103]提出的经验证据表明，在测试时可以去掉很大比例的注意头，而不会显着影响性能。头的数量在不同的层不同-有些层甚至可能只需要一个头。考虑到注意头的冗余性，[103]定义了重要性分数来估计每个头对最终输出的影响，不重要的头可以被删除，以便有效部署。Dalvi等[117]从两个角度分析了预训练Transformer模型中的冗余:通用冗余和特定任务冗余。遵循抽签假设[44]，Prasanna等[117]对BERT中的抽签进行了分析，表明在基于transformer的模型中也存在良好的子网，减少了FFN层和注意力头，以实现较高的压缩率。</p><p>​ 除了transformer模型的宽度外，还可以减少深度(即层的数量)，以加速推理过程[39]。不同于transformer模型中不同的注意头可以并行计算的概念，不同的层必须顺序计算，因为下一层的输入依赖于前一层的输出。Fan等人[39]提出了一种基于层的减少策略来规范模型的训练，然后在测试阶段将整个层一起去除。考虑到不同设备中可用的资源可能不同，Hou等人[62]提出了自适应地减小预定义transformer模型的宽度和深度。该方法同时获得多个不同大小的模型，并通过重链接机制在不同的子网络中共享重要的注意头和神经元。</p><p>​ 除了直接丢弃transformer模型中的模块的剪枝方法之外，矩阵分解的目标是基于低秩假设，用多个小矩阵来近似大矩阵。如Wang等[161]对transformer模型中的标准矩阵乘法进行了分解，提高了推理效率。</p><h4 id="4-6-2-知识蒸馏"><a href="#4-6-2-知识蒸馏" class="headerlink" title="4.6.2 知识蒸馏"></a>4.6.2 知识蒸馏</h4><p>​ 知识蒸馏的目的是通过从大型教师网络转移知识来训练学生网络[59,13,3]。与教师网络相比，学生网络的架构通常更薄、更浅，更容易在资源有限的资源上部署。神经网络的输出特征和中间特征也可以用来将有效的信息从教师传递给学生。慕克吉等人[106]专注于transformer模型，使用预先训练的BERT[34]作为教师，利用大量未标记的数据来指导小模型的训练。Wang等人[156]训练学生网络模仿预先训练的教师模型中的自我注意层的输出。关于值的点积作为一种新的知识形式被引入，以指导学生。在[156]中还引入了教师助理[104]，减少了大型预训练transformer模型和紧凑的学生网络之间的差距，从而促进了模仿。由于transformer模型中存在不同类型的层(即自我注意层、嵌入层和预测层)，Jiao等[73]设计了不同的目标函数来将知识从教师传递给学生。例如，学生模型的嵌入层的输出通过MSE损耗模拟教师的输出。一个可学习的线性变换也被施加到映射不同的特征到同一空间。对于预测层的输出，采用KL散度来度量不同模型之间的差异。</p><h4 id="4-6-3-量化"><a href="#4-6-3-量化" class="headerlink" title="4.6.3 量化"></a>4.6.3 量化</h4><p>​ 量化的目的是减少表示网络权值或中间特征所需的比特数[151,170]。一般神经网络的量化方法已经详细讨论过，并取得了与原始网络相当的性能[112,45,7]。最近，人们对如何对变压器模型进行特殊量化越来越感兴趣[10,40]。例如，Shridhar等人[137]建议将输入嵌入到二进制高维向量中，然后使用二进制输入训练二值神经网络。Cheong等人的[26]用低位(例如4位)表示变压器模型中的权值。Zhao等[188]实证研究了各种量化方法，k-means量化有巨大的发展潜力。Prato等人[118]针对机器翻译任务提出了一种完全量化的transformer，如论文所言，这是第一个翻译质量不受任何损失的8位模型</p><h4 id="4-6-4-紧凑的结构设计"><a href="#4-6-4-紧凑的结构设计" class="headerlink" title="4.6.4 紧凑的结构设计"></a>4.6.4 紧凑的结构设计</h4><p>​ 除了将预定义的Transformer模型压缩成更小的模型之外，有些作品还试图直接设计紧凑的模型[164,72]。Jiang等[72]提出了一种将全连接层和卷积层结合起来的名为跨层动态卷积的新模块，简化了自我注意的计算，如图11所示。通过卷积运算计算不同令牌表示之间的局部依赖性，这比标准变压器中密集的全连接层的效率高得多。深度卷积也被用于进一步降低计算成本。在[2]中提出了有趣的“汉堡包”层，使用矩阵分解代替原来的自我注意层。与标准的自我注意运算相比，矩阵分解能够更有效地计算出不同标记之间的依赖性。高效transformer体系结构的设计也可以通过神经体系结构搜索(neural architecture search, NAS)[52,138]实现自动化。神经体系结构搜索可以自动搜索如何组合不同的组件。</p><p>​ Transformer模型中的自我注意运算是计算给定序列(图像识别任务[37]中的patches)中不同输入令牌表示之间的点积，复杂度为O(N)，其中N为序列的长度。最近，有一个目标是将大型方法的复杂度降低到O(N)，以便变压器模型可以扩展到长序列。例如，Katharopoulos等人[76]将自我注意近似为核特征映射的线性点积，并通过RNNs揭示了令牌之间的关系。Zaheer等人[177]将每个令牌视为图中的一个顶点，并将两个令牌之间的内积计算定义为边。受图论的启发[139,30]，将各种稀疏图结合起来近似transformer模型中的稠密图，可以达到O(N)复杂度。Y un等[175]从理论角度证明了复杂度为O(N)的稀疏变压器足以反映token之间的任何一种关系，并且可以进行普遍逼近，为进一步研究复杂度为O(N)的transformer提供了理论保证。</p><img src="https://gitee.com/leekinghou/image/raw/master/img/20211214175957.png" style="zoom:67%"><center>图 13：图像处理和生成中转换器的通用框架。</center><p><strong>讨论</strong>。上述方法在试图识别transformer模型中的冗余方面采取了不同的方法(参见图12)。</p><p>剪枝和分解方法通常需要预定义的具有冗余的模型。具体来说，剪枝侧重于减少变压器模型中的组件(如层、头)数量，而分解则用多个小矩阵表示原始矩阵。紧凑的模型也可以直接手动设计(需要足够的专业知识)或自动设计(例如，通过Neural Architecture Search NAS)。得到的紧凑模型可以通过量化方法进一步用低比特表示，以便在资源有限的设备上有效部署。</p><h2 id="5-计算机视觉中的自注意力机制"><a href="#5-计算机视觉中的自注意力机制" class="headerlink" title="5. 计算机视觉中的自注意力机制"></a>5. 计算机视觉中的自注意力机制</h2><p>​ 前面的部分回顾了使用 Transformer 架构进行视觉任务的方法。 我们可以得出结论，自注意力在 Transformer 中起着举足轻重的作用。 自注意力模块也可以被认为是 CNN 架构的构建块，其在大感受野方面具有低缩放特性。 该构建块广泛用于网络之上，以捕获远程交互并增强视觉任务的高级语义特征。 在本节中，我们深入研究了为计算机视觉中具有挑战性的任务而设计的基于自注意力的模型。 这些任务包括语义分割、实例分割、对象检测、关键点检测和深度估计。 在这里，我们简要总结了使用自注意力进行计算机视觉的现有应用程序。</p><p><strong>图片分类</strong></p><p>​ 用于分类的可训练注意力包括两个主要流：关于图像区域使用的硬注意力 [3]、[161]、[250] 和软注意力 [227]、[108]、[82]、[184] 生成 非刚性特征图。 巴等人。 [3]首先为图像分类任务提出了术语“视觉注意力”，并使用注意力来选择输入图像内的相关区域和位置。 这也可以降低所提出模型关于输入图像大小的计算复杂度。 对于医学图像分类，提出了 AG-CNN [76] 通过注意力热图从全局图像中裁剪子区域。 而不是使用硬注意力和重新校准特征图的裁剪，SENet [99] 被提议使用软自注意力重新加权卷积特征的通道响应。 杰特利等人。 [108] 使用由相应估计器生成的注意力图来重新加权 DNN 中的中间特征。 此外，Han 等人 [82]利用属性感知注意力来增强CNNs的表示。</p><p><strong>语义片段</strong>。</p><p>​ PSANet [281]、OCNet [262]、DANet [69] 和 CFNet [273] 是提出在语义分割任务中使用自注意力模块的开创性工作。 这些作品考虑并增强了上下文像素之间的关系和相似性 [272]、[130]、[87]、[164]、[236]、[129]。 DANet [69] 同时在空间和通道维度上利用自注意力模块，而 A2Net [35] 将像素分组为一组区域，然后通过将区域表示与生成的注意力权重聚合来增强像素表示。 DGCNet [275] 采用双图 CNN 在单个框架中对坐标空间相似性和特征空间相似性进行建模。 为了提高语义分割的自注意力模块的效率，已经提出了几项工作[264]，[106]，[104]，[131]，[120]，旨在减轻计算带来的大量参数 像素相似度。 例如，CGNL [264] 应用 RBF 核函数的泰勒级数来近似像素相似性。 CCNet [106] 通过两个连续的交叉注意模块近似原始的自我注意方案。 此外，ISSA [104] 将密集亲和矩阵分解为两个稀疏亲和矩阵的乘积。 还有其他相关工作使用基于注意力的图推理模块 [135]、[36]、[131] 来增强局部和全局表示。</p><p>**目标检测 **。</p><p>​ 拉马钱德兰等人 [184] 提出了一个基于注意力的层并交换了传统的卷积层来构建一个完全注意的检测器，该检测器在 COCO 基准测试 [141] 上优于典型的 RetinaNet [140]。 GCNet [18] 假设非局部操作建模的全局上下文对于图像内的不同查询位置几乎相同，并将简化公式和 SENet [99] 统一为全局上下文建模的通用框架 [128]，[ 96]、[62]、[172]。 沃等人 [226] 设计了一个双向操作来收集和分发从查询位置到所有可能位置的信息。 张等人 [271] 表明以前的方法无法与跨尺度特征交互，并提出了基于自我注意模块的特征金字塔变换器，以充分利用跨空间和跨尺度的交互。</p><p>​ 传统的检测方法通常利用单一的视觉表示（例如，边界框和角点）来预测最终结果。 胡等人 [97] 提出了一种基于自注意力的关系模块，通过它们的外观特征之间的交互同时处理一组对象。 程等人。 [41] 提出 RelationNet++ 和桥接视觉表示（BVR）模块，将不同的异构表示组合成一个类似于自我注意模块中的表示。 具体来说，主表示被视为查询输入，辅助表示被视为关键输入。 因此，增强的特征可以桥接来自辅助表示的信息并有益于最终检测结果。</p><p><strong>其他视觉项目</strong>。</p><p>​ 张等人[274] 提出了一种分辨率注意模块，用于在训练多分辨率网络时学习增强的特征图，以获得姿势估计任务的准确人体关键点位置。 此外，Chang 等人 [22] 使用基于注意力机制的特征融合块来提高人体关键点检测模型的准确性。</p><p>​ 为了探索更广义的上下文信息以改进自我监督的单眼训练深度估计，Johnston 等人 [114] 直接利用自注意力模块。 陈等人 [37] 还提出了一种基于注意力的聚合网络来捕获在不同场景中不同的上下文信息以进行深度估计。 和 Aich 等人[1] 提出了双向注意模块，利用前向和后向注意操作以获得更好的单眼深度估计结果。</p><h2 id="6-结论与讨论"><a href="#6-结论与讨论" class="headerlink" title="6. 结论与讨论"></a>6. 结论与讨论</h2><p>​ 与Transformer相比，Transformer以其优异的性能和巨大的潜力成为计算机视觉领域的研究热点。为了发现和利用Transformer的能力，正如本研究中所总结 的，近年来有许多方法被提出。这些方法在广泛的视觉任务，包括骨干，高/中级视觉，低层次视觉，和视频处理方面表现出色。然而，计算机视觉变压器的潜力还没有得到充分的探索，这意味着仍有一些挑战需要解决。在本节中，我们将讨论这些挑战，并对未来的前景提供见解。</p><h3 id="6-1-Challenges"><a href="#6-1-Challenges" class="headerlink" title="6.1. Challenges"></a>6.1. Challenges</h3><p>​ 尽管研究人员已经提出了许多基于Transformer的模型来处理计算机视觉任务，但这些工作只是这个领域的第一步，还有很多改进的空间。例如，ViT[36]中的Transformer架构遵循了NLP的标准变压器[152]，但专门为CV设计的改进版本仍有待探索。此外，有必要将transformer应用于上述任务以外的更多任务。</p><p>​ 计算机视觉Transformer的泛化和鲁棒性也具有挑战性。与cnn相比，纯Transformer缺乏一些归纳偏差，并且严重依赖于大量数据集进行大规模的[36]训练。因此，数据的质量对变压器的泛化和鲁棒性有重要的影响。虽然ViT在CIFAR[79]、VTAB[179]等下游图像分类任务中表现出色，但直接将ViT骨干应用于对象检测并没有达到比CNNs[8]更好的效果。以更一般化的预训练transformer去完成一般化的视觉任务还有很长的路要走。</p><p>​ 尽管有许多工作解释了transformer在自然语言处理中的使用[134,162]，但要清楚地解释为什么transformer在视觉任务中工作得很好仍然是一个具有挑战性的课题。归纳偏差，包括平移均衡性和局部性，归因于CNN的成功，transformer没有任何归纳偏差。目前的文献通常采用直观的方法来分析效果[36,19]。例如，Dosovitskiy等人[36]声称大规模的训练可以超越归纳偏差。在图像补丁中加入位置嵌入来保留位置信息，这在计算机视觉任务中非常重要。受transformer中大量使用参数的启发，过度参数化[100,107]可能是视觉transformer可解释性的一个潜在点。</p><p>​ 最后但并非最不重要的是，为CV开发高效的transformer模型仍然是一个开放的问题。transformer模型通常体积巨大，计算成本昂贵。例如，基本的ViT模型[36]需要180亿次运算才能处理一张图像。相比之下，轻量级的CNN模型GhostNet[54,55]只需约6亿次FLOPs就能实现类似的性能。虽然已经提出了几种压缩transformer的方法，但它们仍然非常复杂。而这些方法原本是为自然语言处理设计的，可能并不适用于CV。因此，迫切需要高效的变压器模型，以便在资源有限的设备上部署可视transformer。</p><h3 id="6-2-未来展望"><a href="#6-2-未来展望" class="headerlink" title="6.2 未来展望"></a>6.2 未来展望</h3><p>​ 为了推动视觉Transformer的发展，我们提出了未来研究的几个潜在方向。</p><p>​ 一个方向是提高Transformer在计算机视觉中的有效性和效率。目标是开发高效的视觉变形器；具体来说，就是高性能、低资源成本的变压器。性能决定了模型是否可以应用到实际应用中，而资源成本则影响在设备上的部署。效果通常与效率相关，如何在两者之间取得更好的平衡是未来研究的一个有意义的课题。</p><p>​ 现有的大多数视觉转换器模型都被设计成只处理单一任务。许多NLP模型，如GPT-3[11]，已经演示了Transformer如何在一个模型中处理多个任务。在CV领域，IPT[20]还能够处理多个低分辨率的视觉任务，如超分辨率、图像去噪、去噪等。我们认为，更多的任务可以包含在一种模型里面。在一个Transformer(即大统一模型)中统一所有视觉任务甚至其他任务是一个令人兴奋的主题。</p><p>​ 神经网络有各种类型，如CNN、RNN和transformer。在CV领域，cnn曾经是主流选择[57,146]，但现在Transformer正变得越来越流行。cnn可以捕获归纳偏差，如转换的空间不变性和局部性，而ViT使用大规模训练来超越归纳偏差[36]。从目前可用的[36]证据来看，cnn在小数据集上表现良好，而Transformer在大数据集上表现更好。未来的问题是使用CNN还是transformer。</p><p>​ 通过使用大数据集进行训练，Transformer可以在NLP[11,34]和CV基准[36]上实现最先进的性能。神经网络可能需要大数据，而不是诱导偏差。最后，我们给您留下一个问题：Transformer 能够通过非常简单的计算范式（例如，仅使用完全连接的层）和大量数据训练获得令人满意的结果吗?</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1] S. Aich, J. M. U. Vianney, M. A. Islam, M. Kaur, and B. Liu. Bidirectional attention network for monocular depth estimation. In IEEE International Conference on Robotics and Automation (ICRA), 2021.</p><p>[2] J. Ba and R. Caruana. Do deep nets really need to be deep? NeurIPS, 27:2654–2662, 2014.</p><p>[3] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In ICLR, 2014.</p><p>[4] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</p><p>[5] T. Bachlechner, B. P. Majumder, H. H. Mao, G. W. Cottrell, and J. McAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint arXiv:2003.04887, 2020.</p><p>[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p><p>[7] Y. Bai, Y.-X. Wang, and E. Liberty. Proxquant: Quantized neural networks via proximal operators. arXiv preprint arXiv:1810.00861, 2018.</p><p>[8] H. Bao, L. Dong, and F. Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.</p><p>[9] A. Bar, X. Wang, V. Kantorov, C. J. Reed, R. Herzig, G. Chechik, A. Rohrbach, T. Darrell, and A. Globerson. Detreg: Unsupervised pretraining with region priors for object detection. arXiv preprint arXiv:2106.04550, 2021.</p><p>[10] J. Beal, E. Kim, E. Tzeng, D. H. Park, A. Zhai, and D. Kislyuk. Toward transformer-based object detection. arXiv preprint arXiv:2012.09958, 2020.</p><p>[11] I. Beltagy, K. Lo, and A. Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019.</p><p>[12] A. Bhandare, V. Sripathi, D. Karkada, V. Menon, S. Choi, K. Datta, and V. Saletore. Efficient 8-bit quantization of transformer neural machine language translation model. arXiv preprint arXiv:1906.00532, 2019.</p><p>[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p><p>[14] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, pages 60–65, 2005.</p><p>[15] C. Buciluaˇ, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541, 2006.</p><p>[16] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In CVPR, pages 6154–6162, 2018.</p><p>[17] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. arXiv preprint arXiv:2105.05537, 2021.</p><p>[18] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In ICCV Workshops, 2019.</p><p>[19] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.</p><p>[20] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, pages 6299–6308, 2017.</p><p>[21] X. Chang, P. Ren, P. Xu, Z. Li, X. Chen, and A. Hauptmann. Scene Graphs: A Survey of Generations and Applications. arXiv:2104.01111 [cs], Mar. 2021.</p><p>[22] Y. Chang, Z. Huang, and Q. Shen. The same size dilated attention net- work for keypoint detection. In International Conference on Artificial Neural Networks, pages 471–483, 2019.</p><p>[23] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond attention visualization. arXiv preprint arXiv:2012.09838, 2020.</p><p>[24] B. Chen, P. Li, C. Li, B. Li, L. Bai, C. Lin, M. Sun, W. Ouyang, et al. Glit: Neural architecture search for global and local image transformer. arXiv preprint arXiv:2107.02960, 2021.</p><p>[25] C.-F. Chen, Q. Fan, and R. Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. arXiv preprintarXiv:2103.14899, 2021.</p><p>[26] C.-F. Chen, R. Panda, and Q. Fan. Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689, 2021.</p><p>[27] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao. Pre-trained image processing transformer. In CVPR, 2021. [28] M. Chen, H. Peng, J. Fu, and H. Ling. Autoformer: Searching transformers for visual recognition. arXiv preprint arXiv:2107.00651, 2021.</p><p>[29] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever.</p><p>Generative pretraining from pixels. In International Conference on</p><p>Machine Learning, pages 1691–1703. PMLR, 2020.</p><p>[30] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam. Diannao: A small-footprint high-throughput accelerator for ubiqui- tous machine-learning. ACM SIGARCH Computer Architecture News, 42(1):269–284, 2014.</p><p>[31] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.</p><p>[32] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.</p><p>[33] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu. Transformer tracking. In CVPR, pages 8126–8135, June 2021.</p><p>[34] Y. Chen, Y. Cao, H. Hu, and L. Wang. Memory enhanced global-local aggregation for video object detection. In CVPR, pages 10337–10346, 2020.</p><p>[35] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng. Aˆ 2-nets: Double attention networks. NeurIPS, pages 352–361, 2018.</p><p>[36] Y. Chen, M. Rohrbach, Z. Yan, Y. Shuicheng, J. Feng, and Y. Kalantidis. Graph-based global reasoning networks. In CVPR, pages 433–442, 2019.</p><p>[37] Y. Chen, H. Zhao, Z. Hu, and J. Peng. Attention-based context aggregation network for monocular depth estimation. International Journal of Machine Learning and Cybernetics, pages 1583–1596, 2021.</p><p>[38] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian. Visformer: The vision-friendly transformer. arXiv preprint arXiv:2104.12533, 2021.</p><p>[39] Y. Cheng, L. Jiang, and W. Macherey. Robust neural machine translation with doubly adversarial inputs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4324– 4333, 2019.</p><p>[40] R. Cheong and R. Daniel. transformers. zip: Compressing transformers with pruning and quantization. Technical report, tech. rep., Stanford University, Stanford, California, 2019.</p><p>[41] C. Chi, F. Wei, and H. Hu. Relationnet++: Bridging visual representa- tions for object detection via transformer decoder. NeurIPS, 2020.</p><p>[42] W. Choi, K. Shahid, and S. Savarese. What are they doing?: Collective activity classification using spatio-temporal relationship among people. In 2009 IEEE 12th international conference on computer vision work- shops, ICCV Workshops, pages 1282–1289. IEEE, 2009.</p><p>[43] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 1(2):3, 2021.</p><p>[44] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021.</p><p>[45] Y.-S. Chuang, C.-L. Liu, and H.-Y. Lee. Speechbert: Cross-modal pre-trained language model for end-to-end spoken question answering. arXiv preprint arXiv:1910.11559, 2019.</p><p>[46] F. Chung and L. Lu. The average distances in random graphs with given expected degrees. Proceedings of the National Academy of Sciences, 99(25):15879–15882, 2002.</p><p>[47] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.</p><p>[48] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. Electra: Pre- training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.</p><p>[49] Z. Dai, B. Cai, Y. Lin, and J. Chen. UP-DETR: unsupervised pre- training for object detection with transformers. In CVPR, 2021.</p><p>[50] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), 2019.</p><p>[51] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, et al. Cogview: Mastering text-to-image generation via transformers. arXiv preprint arXiv:2105.13290, 2021.</p><p>[52] B. Dong, F. Zeng, T. Wang, X. Zhang, and Y. Wei. Solq: Segmenting objects by learning queries. arXiv preprint arXiv:2106.02351, 2021.</p><p>[53] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Unified language model pre-training for natural language understanding and generation. In NeurIPS, pages 13063–13075, 2019.</p><p>[54] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.</p><p>[55] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.</p><p>[56] A. El-Nouby, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek, et al. Xcit: Cross-covariance image transformers. arXiv preprint arXiv:2106.09681, 2021.</p><p>[57] N. Engel, V. Belagiannis, and K. Dietmayer. Point transformer. arXiv preprint arXiv:2011.00931, 2020.</p><p>[58] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high- resolution image synthesis. In CVPR, pages 12873–12883, 2021.</p><p>[59] A. Fan, E. Grave, and A. Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.</p><p>[60] C. Fan. Quantized transformer. Technical report, Technical report, Stanford University, Stanford, California, 2019. URL https . . . .</p><p>[61] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision transformers. arXiv preprint</p><p>arXiv:2104.11227, 2021.</p><p>[62] Q. Fan, W. Zhuo, C.-K. Tang, and Y.-W. Tai. Few-shot object detection with attention-rpn and multi-relation detector. In CVPR, pages 4013–4022, 2020.</p><p>[63] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian. Msg-transformer: Exchanging local spatial information by manipulating</p><p>messenger tokens. arXiv preprint arXiv:2105.15168, 2021.</p><p>[64] Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu. You only look at one sequence: Rethinking transformer in vision through object detection. arXiv preprint arXiv:2106.00666, 2021.</p><p>[65] M. Fayyaz and J. Gall. Sct: Set constrained temporal transformer for set supervised action segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 501–510, 2020.</p><p>[66] R. B. Fisher. Cvonline: The evolving, distributed, non-proprietary, online compendium of computer vision. Retrieved January 28, 2006 from<a target="_blank" rel="noopener" href="http://homepages/">http://homepages</a>. inf. ed. ac. uk/rbf/CVonline, 2008.</p><p>[67] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [68] J. Fromm, M. Cowan, M. Philipose, L. Ceze, and S. Patel. Riptide: Fast end-to-end binarized neural networks. Proceedings of Machine Learning and Systems, 2:379–389, 2020.</p><p>[69] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu. Dual attention network for scene segmentation. In CVPR, pages 3146–3154, 2019. [70] V. Gabeur, C. Sun, K. Alahari, and C. Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214–229, 2020.</p><p>[71] P. Gao, M. Zheng, X. Wang, J. Dai, and H. Li. Fast convergence of detr with spatially modulated co-attention. In ICCV, 2021.</p><p>[72] K. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek. Actor- transformers for group activity recognition. In CVPR, pages 839–848, 2020.</p><p>[73] Z. Geng, M.-H. Guo, H. Chen, X. Li, K. Wei, and Z. Lin. Is attention better than matrix decomposition? In ICLR, 2020.</p><p>[74] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman. Video action transformer network. In CVPR, pages 244–253, 2019.</p><p>[75] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Je ́gou, and M. Douze. Levit: a vision transformer in convnet’s clothing for faster inference. arXiv preprint arXiv:2104.01136, 2021.</p><p>[76] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang. Diagnose like a radiologist: Attention guided convolutional neural network for thorax disease classification. In arXiv preprint arXiv:1801.09927, 2018.</p><p>[77] J. Guo, K. Han, H. Wu, C. Xu, Y. Tang, C. Xu, and Y. Wang. Cmt: Convolutional neural networks meet vision transformers. arXiv preprint arXiv:2107.06263, 2021.</p><p>[78] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu. Pct: Point cloud transformer. Computational Visual Media, 7(2):187– 199, 2021.</p><p>[79] M.-H. Guo, Z.-N. Liu, T.-J. Mu, and S.-M. Hu. Beyond self-attention: External attention using two linear layers for visual tasks. arXiv preprint arXiv:2105.02358, 2021.</p><p>[80] Y. Guo, Y. Zheng, M. Tan, Q. Chen, J. Chen, P. Zhao, and J. Huang. Nat: Neural architecture transformer for accurate and compact architectures. In NeurIPS, pages 737–748, 2019.</p><p>[81] S. Hampali, S. D. Sarkar, M. Rad, and V. Lepetit. Handsformer: Keypoint transformer for monocular 3d pose estimation ofhands and object in interaction. arXiv preprint arXiv:2104.14639, 2021.</p><p>[82] K. Han, J. Guo, C. Zhang, and M. Zhu. Attribute-aware attention model for fine-grained representation learning. In Proceedings of the 26th ACM international conference on Multimedia, pages 2040–2048, 2018.</p><p>[83] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu. Ghostnet: More features from cheap operations. In CVPR, pages 1580–1589, 2020.</p><p>[84] K. Han, Y. Wang, Q. Zhang, W. Zhang, C. Xu, and T. Zhang. Model ru- bik’s cube: Twisting resolution, depth and width for tinynets. NeurIPS, 33, 2020.</p><p>[85] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.</p><p>[86] M. Hao, Y. Li, Z. Di, N. B. Gundavarapu, and X. Wang. Test-time personalization with a transformer for human pose estimation. arXiv preprint arXiv:2107.02133, 2021.</p><p>[87] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao. Adaptive pyramid context network for semantic segmentation. In CVPR, pages 7519– 7528, 2019.</p><p>[88] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 9729– 9738, 2020.</p><p>[89] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. pages 770–778, 2016.</p><p>[90] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang. TransReID: Transformer-based object re-identification. In ICCV, 2021.</p><p>[91] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.</p><p>[92] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh. Rethinking spatial dimensions of vision transformers. In ICCV, 2021.</p><p>[93] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p><p>[94] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.</p><p>[95] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu. Dynabert: Dynamic bert with adaptive width and depth. NeurIPS, 33, 2020.</p><p>[96] T.-I. Hsieh, Y.-C. Lo, H.-T. Chen, and T.-L. Liu. One-shot object detection with co-attention and co-excitation. In NeurIPS, pages 2725– 2734, 2019.</p><p>[97] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks for object detection. In CVPR, pages 3588–3597, 2018.</p><p>[98] J. Hu, L. Cao, Y. Lu, S. Zhang, Y. Wang, K. Li, F. Huang, L. Shao, and R. Ji. Istr: End-to-end instance segmentation with transformers. arXiv preprint arXiv:2105.00637, 2021.</p><p>[99] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In CVPR, pages 7132–7141, 2018.</p><p>[100] R. Hu and A. Singh. Unit: Multimodal multitask learning with a unified transformer. arXiv preprint arXiv:2102.10772, 2021.</p><p>[101] K. Huang, J. Altosaar, and R. Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019.</p><p>[102] L. Huang, J. Tan, J. Liu, and J. Yuan. Hand-transformer: Non- autoregressive structured modeling for 3d hand pose estimation. In ECCV, pages 17–33, 2020.</p><p>[103] L. Huang, J. Tan, J. Meng, J. Liu, and J. Yuan. Hot-net: Non- autoregressive transformer for 3d hand-object pose estimation. In Proceedings of the 28th ACM International Conference on Multimedia, pages 3136–3145, 2020.</p><p>[104] L. Huang, Y. Yuan, J. Guo, C. Zhang, X. Chen, and J. Wang. Inter- laced sparse self-attention for semantic segmentation. arXiv preprint arXiv:1907.12273, 2019.</p><p>[105] Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021.</p><p>[106] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, pages 603– 612, 2019.</p><p>[107] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.</p><p>[108] S. Jetley, N. A. Lord, N. Lee, and P. H. Torr. Learn to pay attention. In ICLR, 2018.</p><p>[109] D. Jia, K. Han, Y. Wang, Y. Tang, J. Guo, C. Zhang, and D. Tao. Ef- ficient vision transformers via fine-grained manifold distillation. arXiv preprint arXiv:2107.01378, 2021.</p><p>[110] T. Jiang, N. C. Camgoz, and R. Bowden. Skeletor: Skeletal transformers for robust body-pose estimation. In CVPR, pages 3394–3402, 2021.</p><p>[111] Y. Jiang, S. Chang, and Z. Wang. Transgan: Two transformers can make one strong gan. arXiv preprint arXiv:2102.07074, 2021.</p><p>[112] Z.-H. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan. Convbert: Improving bert with span-based dynamic convolution. NeurIPS, 33, 2020.</p><p>[113] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174, Nov. 2020.</p><p>[114] A. Johnston and G. Carneiro. Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. In CVPR, pages 4756–4765, 2020.</p><p>[115] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64–77, 2020.</p><p>[116] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156–5165. PMLR, 2020.</p><p>[117] R. Kimchi, M. Behrmann, and C. R. Olson. Perceptual organization in vision: Behavioral and neural perspectives. Psychology Press, 2003.</p><p>[118] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.</p><p>[119] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, pages 1097– 1105, 2012.</p><p>[120] S. Kumaar, Y. Lyu, F. Nex, and M. Y. Yang. Cabinet: Efficient context aggregation network for low-latency semantic segmentation. arXiv preprint arXiv:2011.00993, 2020.</p><p>[121] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representa- tions. arXiv preprint arXiv:1909.11942, 2019.</p><p>[122] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</p><p>[123] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240, 2020.</p><p>[124] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and compre- hension. arXiv preprint arXiv:1910.13461, 2019.</p><p>[125] C. Li, T. Tang, G. Wang, J. Peng, B. Wang, X. Liang, and X. Chang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self- supervised neural architecture search. arXiv preprint arXiv:2103.12424, 2021.</p><center>翻译：lijinhao</center><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/leekinghou/image/raw/master/img/20211214202024.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/12/15/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><img class="prev-cover" src="https://gitee.com/leekinghou/image/raw/master/img/20211124181101.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">动态规划</div></div></a></div><div class="next-post pull-right"><a href="/2021/12/10/offer04/"><img class="next-cover" src="https://gitee.com/leekinghou/image/raw/master/img/20211124181101.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">剑指 Offer 04. 二维数组中的查找</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/11/21/AlexNet/" title="深度卷积神经网络（AlexNet）"><img class="cover" src="https://gitee.com/leekinghou/image/raw/master/img/20211124225058.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-21</div><div class="title">深度卷积神经网络（AlexNet）</div></div></a></div><div><a href="/2021/11/23/NIN%EF%BC%88%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%89/" title="NIN（网络中的网络）"><img class="cover" src="https://gitee.com/leekinghou/image/raw/master/img/20211125164459.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-23</div><div class="title">NIN（网络中的网络）</div></div></a></div><div><a href="/2021/11/22/VGG/" title="VGG块网络"><img class="cover" src="https://gitee.com/leekinghou/image/raw/master/img/20211125104107.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-22</div><div class="title">VGG块网络</div></div></a></div><div><a href="/2021/11/25/googlenet/" title="含并行连结的网络（GoogLeNet）"><img class="cover" src="https://gitee.com/leekinghou/image/raw/master/img/20211125181324.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-25</div><div class="title">含并行连结的网络（GoogLeNet）</div></div></a></div></div></div></article></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/portrait.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">lijinhao</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">38</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">21</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/leekinghou"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/leekinghou" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lijinhao716@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">醉后不知天在水，满船清梦压星河✨<div class="twopeople"><div class="twopeople"><div class="container" style="height:200px"><canvas class="illo" width="800" height="800" style="max-width:200px;max-height:200px;touch-action:none;width:640px;height:640px"></canvas></div><script src="https://cdn.guole.fun/js/twopeople1.js"></script><script src="https://cdn.guole.fun/js/zdog.dist.js"></script><script id="rendered-js" src="https://cdn.guole.fun/js/twopeople.js"></script><style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E8%A7%86%E8%A7%89Transformers%E7%9A%84%E8%B0%83%E6%9F%A5"><span class="toc-number">1.</span> <span class="toc-text">对于视觉Transformers的调查</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">1. 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Transformer%E7%9A%84%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text">2. Transformer的公式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%B8%B8%E8%A7%84%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 自注意力机制的常规公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%BC%A9%E6%94%BE%E7%9A%84%E7%82%B9%E4%B9%98%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 缩放的点乘自注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Transformer%E4%B8%AD%E7%9A%84%E5%85%B6%E4%BB%96%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3 Transformer中的其他关键概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%87%8D%E6%B8%A9-Transformers%E5%BA%94%E7%94%A8%E4%BA%8ENLP"><span class="toc-number">1.4.</span> <span class="toc-text">3. 重温 Transformers应用于NLP</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E8%A7%86%E8%A7%89-Transformer"><span class="toc-number">2.</span> <span class="toc-text">4. 视觉 Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%BB%E5%B9%B2"><span class="toc-number">2.1.</span> <span class="toc-text">4.1 表征学习的主干</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%AFTransformer"><span class="toc-number">2.1.0.1.</span> <span class="toc-text">纯Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-Transformer-%E4%B8%8E%E5%8D%B7%E7%A7%AF"><span class="toc-number">2.1.0.2.</span> <span class="toc-text">4.1.2 Transformer 与卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.1.0.3.</span> <span class="toc-text">4.1.3 自监督表征学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA"><span class="toc-number">2.1.1.</span> <span class="toc-text">讨论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E9%AB%98-%E4%B8%AD%E5%B1%82%E8%A7%86%E8%A7%89"><span class="toc-number">2.1.2.</span> <span class="toc-text">4.2 高&#x2F;中层视觉</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-%E5%B8%B8%E8%A7%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">4.2.1 常规目标检测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">姿态估计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5-%E8%AE%A8%E8%AE%BA"><span class="toc-number">2.1.3.</span> <span class="toc-text">4.2.5 讨论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Low-level-Vision"><span class="toc-number">2.1.4.</span> <span class="toc-text">4.3 Low-level Vision</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90"><span class="toc-number">2.1.4.1.</span> <span class="toc-text">4.3.1 图片生成</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E9%AB%98%E6%95%88-Transformer"><span class="toc-number">2.1.5.</span> <span class="toc-text">4.6. 高效 Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-1-Pruning-and-Decomposition"><span class="toc-number">2.1.5.1.</span> <span class="toc-text">4.6.1 Pruning and Decomposition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-2-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-number">2.1.5.2.</span> <span class="toc-text">4.6.2 知识蒸馏</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-3-%E9%87%8F%E5%8C%96"><span class="toc-number">2.1.5.3.</span> <span class="toc-text">4.6.3 量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-4-%E7%B4%A7%E5%87%91%E7%9A%84%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.1.5.4.</span> <span class="toc-text">4.6.4 紧凑的结构设计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">2.2.</span> <span class="toc-text">5. 计算机视觉中的自注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%BB%93%E8%AE%BA%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="toc-number">2.3.</span> <span class="toc-text">6. 结论与讨论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Challenges"><span class="toc-number">2.3.1.</span> <span class="toc-text">6.1. Challenges</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B"><span class="toc-number">2.3.2.</span> <span class="toc-text">6.2 未来展望</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E7%94%A8"><span class="toc-number">2.4.</span> <span class="toc-text">引用</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By lijinhao</div><div class="framework-info"><span>Framework</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">满腔期待 常怀热情 有人等你<a target="_blank" rel="noopener" href="https://inews.gtimg.com/newsapp_ls/0/14146688226/0">Baikal♥️Swan</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"></div><script defer id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="false" data-click="false"></script><script src="https://cdn-1.nesxc.com/js/smooth-scrolling.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script><script data-pjax>function GithubCalendarConfig(){var e=document.getElementById("recent-posts");e&&"/"==location.pathname&&(console.log("已挂载github calendar"),e.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>')),GithubCalendar("https://python-github-calendar-api.vercel.app/api?Leekinghou",["#ebedf0","#fdcdec","#fc9bd9","#fa6ac5","#f838b2","#f5089f","#c4067e","#92055e","#540336","#48022f","#30021f"],"Leekinghou")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:0}}</style><style></style></body></html>