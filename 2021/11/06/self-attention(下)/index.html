<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Self-attention(下) | lijinhao's blog</title><meta name="keywords" content="ML"><meta name="author" content="lijinhao"><meta name="copyright" content="lijinhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="self-attention —— 天涯若比邻；self-attention与cnn、gnn对比；论文推荐"><meta property="og:type" content="article"><meta property="og:title" content="Self-attention(下)"><meta property="og:url" content="https://lijinhao.site/2021/11/06/self-attention(%E4%B8%8B)/index.html"><meta property="og:site_name" content="lijinhao&#39;s blog"><meta property="og:description" content="self-attention —— 天涯若比邻；self-attention与cnn、gnn对比；论文推荐"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/14146047881/0"><meta property="article:published_time" content="2021-11-05T16:00:00.000Z"><meta property="article:modified_time" content="2022-06-20T12:15:35.572Z"><meta property="article:author" content="lijinhao"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Self-attention"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://inews.gtimg.com/newsapp_ls/0/14146047881/0"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lijinhao.site/2021/11/06/self-attention(%E4%B8%8B)/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"Copy successfully",error:"Copy error",noSupport:"The browser does not support"},relativeDate:{homepage:!1,post:!1},runtime:"days",date_suffix:{just:"Just",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Self-attention(下)",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-06-20 20:15:35"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));GLOBAL_CONFIG_SITE.isHome&&/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom@1.0.0.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/portrait.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">24</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i> <span>清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i> <span>照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">lijinhao's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i> <span>清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i> <span>照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Self-attention(下)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-11-05T16:00:00.000Z" title="Created 2021-11-06 00:00:00">2021-11-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-06-20T12:15:35.572Z" title="Updated 2022-06-20 20:15:35">2022-06-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Self-attention(下)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><p>前言：self-attention(上)提到了<strong>从一排向量中获得$b^i$的方法，$b^i$用以判断各个向量的关联程度</strong></p><p>需要强调的一点是，$b^1$到$b^4$的计算是并行开始的，没有前后依赖关系，这一点也是self-attention区分于RNN(循环神经网络)的重要特征。</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146106462/0"></p><h2 id="矩阵角度分析-b-i-的计算过程"><a href="#矩阵角度分析-b-i-的计算过程" class="headerlink" title="矩阵角度分析$b^i$的计算过程"></a>矩阵角度分析$b^i$的计算过程</h2><p>我们已经得知每一个$a$都会产生$q、k、v$</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146124263/0" alt="image.png"></p><ol><li><p>求得$q、k、v$</p><p>$a$ 作为query时，按照流程我们要将每一个$a$都乘上一个矩阵$W^q$（是什么东西？后面提到。但可以提前说一句$W^q$<strong>其实是 network 的参数，它是在后面由学习得到的</strong>），用于求得 $q$</p><p>既然是并行的，那么就可以将这个过程组成矩阵运算，同理求 $k$、 $v$也是如此</p></li></ol><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146161460/0" alt="1636170809140.png"></p><ol start="2"><li><p>求得$a’$</p><p>下一步是<strong>每一个 $k$的转置（横框）与每一个 $q$，计算inner product，得到该attention 的分数</strong>：</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146183003/0" alt="1636171303495.png"></p><p>​ 那这四个步骤的操作，可以把它拼起来，看作是<strong>矩阵跟向量相乘</strong>。于是得到 attention 分数这个步骤，如果从矩阵操作的角度来看就是以 $k^1$ 到 $k^4$ 做列，$q^1$做行进行运算：</p></li></ol><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146196476/0" alt="image.png"></p><p>​ 推广到对$k^1$ 到 $k^4$ 计算 attention，$q^2$，$q^3$ ，$q^4$也要对 $k^1$ 到 $k^4$ 计算 attention。</p><p>​ 我们会在 attention 的分数基础上<strong>进行 normalization操作</strong>，比如说<strong>对每一个 column做 softmax，让每一个 column 里面的值相加是 1</strong>。</p><p>​ 上篇有提到过做 <strong>softmax不是唯一的选项</strong>，你完全可以选择其他的操作，比如说 ReLU 。此处通过了 softmax 以后，$a$ 的值就发生了变化，所以我们用$a′$来表示通过 softmax 以后的结果。</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146227160/0" alt="1636172049290.png"></p><ol start="3"><li><p>求得 $b^i$ （加权求和）</p><p>我们把 $v^1$ 到 $v^4$ 乘上的 α 以后，就可以得到 $b$</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146241294/0"></p><p>​ 关于矩阵 $O$，矩阵中的每一个column，都是self-attention的输出，也就是 $b^1$ 到 $b^4$</p></li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总结以上矩阵乘法，就是：</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14146649078/0"></p><ul><li>$I$ 是self-attention的输入(input)，输入值是一排向量(vector)，这排向量拼接起来当作矩阵的column就是$I$</li><li>输入值 $I$ 分别乘上三个矩阵，$W^q , W^k, W^v$ 可以得到矩阵$Q 、 K、 V$</li><li>对于这三个矩阵$Q 、 K、 V$，接下来 $Q$ 乘以 $K$ 的转置，得到矩阵 $A$，对矩阵 $A$ 进行一些处理后得到 $A’$，我们叫做<strong>Attention Matrix</strong>，生成矩阵 $Q$ 就是为了得到Attention的score。</li><li>**生成矩阵 $V$ 是为了计算最后的 $b$，也就是矩阵 $O$ **，然后接下来把 $A′$ 再乘上 $V$，就得到 $O$ 就是 <strong>Self-attention 这一层的输出</strong></li></ul><p>此时应该可以明白$W^q , W^k, W^v$ 是什么东西了，也就是训练数据集中数据转化而成的参数。</p><p>综上，从 $I$ 到 $O$ 就是做了 Self-attention。</p><h1 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Multi-head self-attention是self-attention的进阶版本。</p><p>回忆前面，$q$ 代表Query，它就像是使用搜寻引擎，去搜寻相关文章时的<strong>关键字</strong>。我们使用 $q$ 去寻找相关的 $k$ ，但是“相关”这个词有很多种形式，也有不同的方式去定义，因此可以<strong>定义多个 $q$ ，我们可以定义不同的 $q$​​​ ，去负责不同种类的相关性。</strong></p><h2 id="具体操作方法"><a href="#具体操作方法" class="headerlink" title="具体操作方法"></a>具体操作方法</h2><p>$q$ 有两个，那 $k$ 、 $v$ 也就要有两个。分别从 $q$ 、$k$ 、$v$ 得到 $q^1$、$q^2$， $k^1$、$k^2$，$v^1$、$v^2$。</p><p>那其实就是把 $q$ 、 $k$ 、 $v$，分别乘上两个矩阵，得到不同的 head</p><p>对另外一个位置，也做一样的事情。总之就是$q^1$在算attention 的分数的时候，就不用考虑 $k^2$了。</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14147720870/0" alt="image.png"></p><p>最后得到的$b^i$跟普通的self-attention一样，也只有一个：</p><p><img src="https://inews.gtimg.com/newsapp_ls/0/14147736106/0" alt="1636202296302.png"></p><p>也就是再乘上一个矩阵，然后得到 $b^i$，然后再送到下一层去，这个就是 Multi-head attention —— Self-attention 的变形。</p><h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>对 Self-attention 而言，“天涯若比邻”恰如其分，所有的向量之间的距离都是一样的。</p><p>目前为止文章中提到的Self-attention都没有考虑向量的位置因素，在某些机遇深度学习的项目中，向量的位置可能有重要作用。例如说词性标注这个例子，也许<strong>动词出现在句首的可能性不大</strong>，所以如果我们看到某一个词汇它是放在句首的，那它是动词的可能性可能就比较低，此时的位置信息是有用的。</p><p><strong>question：那么我们怎么提现向量的位置呢？</strong></p><h2 id="位置向量-e-i"><a href="#位置向量-e-i" class="headerlink" title="位置向量$e^i$"></a>位置向量$e^i$</h2><p>为每一个位置设定一个 vector，叫做 positional vector。<strong>每一个不同的位置</strong>，就有不同的 vector。</p><p><img src="https://pic.rmb.bdstatic.com/bjh/12768eb7241326f75844efc06d81566b.png" alt="1636202772325.png"></p><p>在《 Attention Is All You Need 》这篇论文中，它用的位置向量$e^i$如下所示：</p><p><img src="https://pic.rmb.bdstatic.com/bjh/a9b1d61c9bd357c25db2eee506bba4fd.png" alt="1636203008754.png"></p><p>图中每一个 column 代表一个 $e$，第一个位置就是 $e^1$，第二个是位置 $e^2$，第三个位置就是 $e^3$，以此类推……</p><p>当模型在处理输入向量的时候，就可以根据 $e^i$ 知道对应的向量的位置信息。</p><p>实际上这个位置向量是人为规定的，那么可能会出现很多问题，如果只定到128位，那么当序列长度出现到129时就会出现问题</p><p>在《Attention Is All You Need paper》这篇论文里，并不存在这个问题，它的向量是由某一个规则产生的，经过一个sin、cos的函数后，得到了位置信息</p><p>可创新点：我们在进行一项新的研究时大可以发挥想象力，创造出自己的方法，甚至从数据集中学习得到。当前<strong>positional encoding仍然是一个尚待研究的问题，你永远可以提出新的做法</strong>（2021-11-06）。</p><p>推荐一篇论文，里面提出了新的positional encoding方法：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.09229#">Learning to Encode Position for Transformer with Continuous Dynamical Model</a></p><p><img src="https://pic.rmb.bdstatic.com/bjh/5f06628d028263574b1b4f6dd2a44a4d.png" alt="image.png"></p><p><img src="https://pic.rmb.bdstatic.com/bjh/8768ef6b3a95551876baf7c057262cf3.png" alt="1636207412788.png"></p><h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><h2 id="NLP-领域"><a href="#NLP-领域" class="headerlink" title="NLP 领域"></a>NLP 领域</h2><p>在 NLP 领域有一个东西叫做 BERT，BERT 里面也用到 Self-attention，所以 Self-attention 在 NLP 上面的应用是比较广泛的</p><ul><li>但是在用于自然语言处理时，self-attention的输入向量要做一些处理</li></ul><p>因为由语音组成的声音信号排成一排向量时，这排向量可能会非常非常长。而每一个向量，仅仅代表了10毫米的声音信息，所以如果是 1 秒钟的声音信号，它就有 100 个向量，5 秒钟的声音，就 500 个向量，随便讲一句话，都是上千个向量。</p><p><img src="https://pic.rmb.bdstatic.com/bjh/3544b0d0d9763125a510f56840c27857.png" alt="1636207477173.png"></p><p>那么对于self-attention——考虑输入序列所有的向量，就非常难顶了。</p><p>实际上我们可以知道<strong>计算attention matrix 的时候，它的计算complexity 是输入向量长度的平方</strong></p><p><img src="https://pic.rmb.bdstatic.com/bjh/acad4dea04ac12e2f5d981d2cbd1fb26.png" alt="1636207654024.png"></p><p>如果要按照原方案，考虑所有的向量，这就要求我们有足够大的内存，才能把这一整个矩阵存下来，这是不现实的。</p><p>所以做语音处理的时候，有一招叫做<strong>Truncated Self-attention</strong></p><h3 id="Truncated-Self-attention"><a href="#Truncated-Self-attention" class="headerlink" title="Truncated Self-attention"></a>Truncated Self-attention</h3><p>Truncated Self-attention 做的事情就是，在做 Self-attention 的时候，<strong>不要看一整句话，就只看一个小的范围就好</strong></p><p><img src="https://pic.rmb.bdstatic.com/bjh/de2ed8b92b848dc9a152645d4939f8c8.png" alt="1636207779132.png"></p><p>那至于<strong>这个范围应该要多大？那看个人设定了，取决于你对这个问题的理解</strong></p><h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><p>我们都说 <strong>Self-attention 适用的范围是：输入是一个向量集的时候</strong></p><p>一张图片不就是一个很长的向量嘛？然后加上RGB通道的三层之后，就是一个向量集。</p><p><img src="https://pic.rmb.bdstatic.com/bjh/a71c434e6efc17e9c6f5b9530660d859.png" alt="1636207985162.png"></p><p>这个是一个解析度 5 *10 的图片，图片可以看作是一个 tensor，这个 tensor 的大小是 5 * 10 * 3，3 代表 RGB 三个通道。</p><p>把每一个位置的像素都看作是一个三维的向量，所以<strong>每一个像素就是一个三维的向量</strong>，那<strong>整张图片，就是 5 * 10 个向量的集合</strong></p><h3 id="两个使用self-attention处理图像的例子"><a href="#两个使用self-attention处理图像的例子" class="headerlink" title="两个使用self-attention处理图像的例子"></a>两个使用self-attention处理图像的例子</h3><ol><li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.08318">Self-Attention Generative Adversarial Networks</a></li></ol><p><img src="https://pic.rmb.bdstatic.com/bjh/828f81c9a22db2fb4c1f966d2138f73f.png" alt="1636211029907.png"></p><p><img src="https://pic.rmb.bdstatic.com/bjh/f2b06022c08c1f3a795123a1e74bedb9.png" alt="1636211093836.png"></p><ol start="2"><li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers</a></li></ol><p><img src="https://pic.rmb.bdstatic.com/bjh/19f9f2c8f9e06daac23f4992e55c12ef.png" alt="1636466310458.png"></p><p><img src="https://pic.rmb.bdstatic.com/bjh/7ebf67c9e544fb636b2379b928d94df3.png" alt="1636466234744.png"></p><h1 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h1><p>Self-attention 和 CNN、RNN相比如何？</p><h2 id="Self-attention-v-s-CNN"><a href="#Self-attention-v-s-CNN" class="headerlink" title="Self-attention v.s. CNN"></a>Self-attention v.s. CNN</h2><p><img src="https://pic.rmb.bdstatic.com/bjh/674e2bddebaec9d2cd97702f8c7bbbda.png" alt="1636467328481.png"></p><p>​</p><h2 id="差异性"><a href="#差异性" class="headerlink" title="差异性"></a>差异性</h2><ul><li><p>输入数据的差异</p><ul><li>对于一张图片，如果用Self-attention进行处理，那么输入模型的向量就是一个个像素，要考虑的其中一个像素产生query时，其他像素产生key。同时要考虑一整张图片的信息。如果用CNN，就会画出一个receptive field，每一个神经元只考虑这个区域里的信息。</li><li>所以如果比较 CNN 跟 Self-attention，<strong>CNN 可以看作是一种简化版的 Self-attention</strong>，因为在做CNN的时候，我们只考虑 receptive field 里面的信息，而在做 Self-attention 的时候，我们是考虑整张图片的信息。</li></ul></li><li><p>学习区域的差异</p><ul><li>在 CNN 里面，我们要划定 receptive field，每一个神经元，只考虑 receptive field 里面的信息。receptive field 的<strong>范围</strong>跟<strong>大小</strong>，是人为决定的。</li><li>而对 Self-attention 而言，我们用 attention，去找出相关的 像素，就好像是 <strong>receptive field 是自动学习</strong>，network 自己决定receptive field 的形状长什么样子，network 自己决定以哪个像素为中心，哪些像素是真正需要考虑的，哪些pixel是相关的</li></ul></li></ul><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.03584">On the Relationship,between Self-attention and Convolutional Layers</a></p><p>在这篇 paper 里会用数学的方式严谨的告诉你，其实<strong>CNN就是 Self-attention 的特例，Self-attention 只要设定合适的参数，它可以做到跟 CNN 一模一样的事情</strong></p><p>所以<strong>self-attention，是更 灵活 的 CNN，而 CNN 是有受限制的 Self-attention</strong></p><p><img src="https://pic.rmb.bdstatic.com/bjh/5eed518e810030990ea26196995711c1.png" alt="image.png"></p><h2 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h2><p>既然Self-attention比较灵活，那么我们知道<strong>比较灵活的模型，需要更多的数据,如果数据量不够就有可能overfitting</strong></p><p>而小的模型（限制比较多的模型，它适合在数据量少的时候使用，因为它可能不容易overfitting，所以就提供了一种构建模型的思路，<strong>如果模型的限制设计得比较好，就会有不错的结果</strong>。</p><p>下面一篇论文比较了用不同的 data 量来训练 CNN 跟 Self-attention时的效果：</p><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929">An Image is Worth 16x16 Words Transformers for Image Recognition at scale</a></p><p><img src="https://pic.rmb.bdstatic.com/bjh/65249ff9dfd2ddc04ed462e96b8c4ee3.png" alt="1636468907197.png"></p><h2 id="Self-attention-v-s-RNN"><a href="#Self-attention-v-s-RNN" class="headerlink" title="Self-attention v.s. RNN"></a>Self-attention v.s. RNN</h2><p>RNN：**上一个输入序列的输出用作下一个输入序列的输入 **——慢（但是 <strong>RNN 其实也可以是双向的</strong>）</p><p><img src="https://pic.rmb.bdstatic.com/bjh/dd37c2c476e60c6779b34b1958701db9.png" alt="1636469093979.png"></p><p><img src="https://pic.rmb.bdstatic.com/bjh/f32b8954caf882fd86b163e8a93aaeb7.png" alt="1636469229956.png"></p><h2 id="Self-attention-for-Graph"><a href="#Self-attention-for-Graph" class="headerlink" title="Self-attention for Graph"></a>Self-attention for Graph</h2><p>图也可以看作是一堆向量，既然是一堆向量那么就可以用 Self-attention 来处理。所以 <strong>Self-attention 也可以用在图上面</strong></p><p>但是把 Self-attention用在Graph 上面的时候有特别之处</p><p><img src="https://pic.rmb.bdstatic.com/bjh/c524cc32f8d8f19e4964ca6e09bd99c7.png" alt="1636469397128.png"></p><p>在Graph上面，每一个节点可以表示成一个向量。但<strong>不只有节点的信息，还有边的信息</strong>，我们知道哪些节点之间是相连的，也就是哪些节点是有关联的。</p><p>有了边的信息，那么节点之间的关联性就不需要通过算法学习的出，可以直接拿到节点之间的关联性。也就是说，使用<strong>Self-attention做Attention Matrix计算时，只需要计算有边相连的节点就好</strong></p><p>若两个节点之间没有关系，<strong>我们就不需要再去计算它的 attention score，直接把它设为 0 就好</strong></p><p>这种限制，用在 Graph 上面的时候，其实就是一种 Graph Neural Network，也就是一种 GNN。</p><p><img src="https://pic.rmb.bdstatic.com/bjh/2c9df7989cd1e8ad7995e6e7096cf1ca.png" alt="1636470716308.png"></p><h1 id="More"><a href="#More" class="headerlink" title="More"></a>More</h1><p>Self-attention 有非常多的变形，论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.04006">《Long Range Arena: A Benchmark for Efficient Transformers》</a>和<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.06732">《Efficient Transformers: A Survey 》</a>里面做出了比较。</p><p><img src="https://pic.rmb.bdstatic.com/bjh/7072fbd0ba959f4e8d5713eb9d2d787c.png" alt="1636471112210.png"></p><p>因为 Self-attention 最大的问题就是<strong>运算量非常地大</strong>，所以怎么样减少 Self-attention 的运算量，是一个未来的重点。</p><p>往右代表算法运算的速度，它们的速度会比原来的 Transformer 快，但是快的速度带来的就是performance 变差</p><p>这个纵轴代表是 performance,所以它们往往比原来的 Transformer表现得差一点，但是速度会比较快。</p></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a><a class="post-meta__tags" href="/tags/Self-attention/">Self-attention</a></div><div class="post_share"><div class="social-share" data-image="https://inews.gtimg.com/newsapp_ls/0/14146047881/0" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/11/06/Softmax/"><img class="prev-cover" src="https://pic.rmb.bdstatic.com/bjh/13db446ce99ca773b08b7f121cd0d42b.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">SoftMax</div></div></a></div><div class="next-post pull-right"><a href="/2021/11/03/2021-10-11-%E7%BB%88%E7%AB%AF%E5%88%A9%E5%99%A8-tmux/"><img class="next-cover" src="https://inews.gtimg.com/newsapp_ls/0/14137230625/0" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">终端利器——tmux</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/11/01/self-attention(%E4%B8%8A)/" title="Self-attention(上)"><img class="cover" src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111636.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-01</div><div class="title">Self-attention(上)</div></div></a></div><div><a href="/2021/09/28/2021-09-28-sklearn%E4%BD%BF%E7%94%A8%E5%87%BD%E6%95%B0%E6%96%87%E6%A1%A3/" title="sklearn使用函数文档"><img class="cover" src="https://files.catbox.moe/nqxbpk.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-28</div><div class="title">sklearn使用函数文档</div></div></a></div><div><a href="/2021/08/28/C1/" title="MACHINE LEARNING"><img class="cover" src="https://files.catbox.moe/53u8pl.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-28</div><div class="title">MACHINE LEARNING</div></div></a></div><div><a href="/2021/09/22/CNN/" title="CNN卷积神经网络笔记"><img class="cover" src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/20220620220530.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-22</div><div class="title">CNN卷积神经网络笔记</div></div></a></div><div><a href="/2021/09/26/K%E8%BF%91%E9%82%BB%20K-nearest%20neighbor/" title="K近邻 K-nearest neighbor"><img class="cover" src="https://files.catbox.moe/yechci.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-26</div><div class="title">K近邻 K-nearest neighbor</div></div></a></div><div><a href="/2021/10/20/lessonOne/" title="李宏毅2021年(春)深度学习 作业一"><img class="cover" src="https://inews.gtimg.com/newsapp_ls/0/14146721068/0" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-20</div><div class="title">李宏毅2021年(春)深度学习 作业一</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/portrait.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">lijinhao</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">24</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/leekinghou"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/leekinghou" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lijinhao716@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">醉后不知天在水，满船清梦压星河✨<div class="twopeople"><div class="twopeople"><div class="container" style="height:200px"><canvas class="illo" width="800" height="800" style="max-width:200px;max-height:200px;touch-action:none;width:640px;height:640px"></canvas></div><script src="https://cdn.guole.fun/js/twopeople1.js"></script><script src="https://cdn.guole.fun/js/zdog.dist.js"></script><script id="rendered-js" src="https://cdn.guole.fun/js/twopeople.js"></script><style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E8%A7%92%E5%BA%A6%E5%88%86%E6%9E%90-b-i-%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">矩阵角度分析$b^i$的计算过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">2.</span> <span class="toc-text">小结</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#Multi-head-Self-attention"><span class="toc-number"></span> <span class="toc-text">Multi-head Self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">具体操作方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number"></span> <span class="toc-text">Positional Encoding</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E5%90%91%E9%87%8F-e-i"><span class="toc-number">1.</span> <span class="toc-text">位置向量$e^i$</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Applications"><span class="toc-number"></span> <span class="toc-text">Applications</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#NLP-%E9%A2%86%E5%9F%9F"><span class="toc-number">1.</span> <span class="toc-text">NLP 领域</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Truncated-Self-attention"><span class="toc-number">1.1.</span> <span class="toc-text">Truncated Self-attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">图像处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E4%BD%BF%E7%94%A8self-attention%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.1.</span> <span class="toc-text">两个使用self-attention处理图像的例子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AF%94%E8%BE%83"><span class="toc-number"></span> <span class="toc-text">比较</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-attention-v-s-CNN"><span class="toc-number">1.</span> <span class="toc-text">Self-attention v.s. CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%AE%E5%BC%82%E6%80%A7"><span class="toc-number">2.</span> <span class="toc-text">差异性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E6%80%A7"><span class="toc-number">3.</span> <span class="toc-text">适用性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-attention-v-s-RNN"><span class="toc-number">4.</span> <span class="toc-text">Self-attention v.s. RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-attention-for-Graph"><span class="toc-number">5.</span> <span class="toc-text">Self-attention for Graph</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#More"><span class="toc-number"></span> <span class="toc-text">More</span></a></li></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By lijinhao</div><div class="framework-info"><span>Framework</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">满腔期待 常怀热情 有人等你<a target="_blank" rel="noopener" href="https://inews.gtimg.com/newsapp_ls/0/14146688226/0">Baikal♥️Swan</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"></div><script defer id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="false" data-click="false"></script><script src="https://cdn-1.nesxc.com/js/smooth-scrolling.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script><script data-pjax>function GithubCalendarConfig(){var e=document.getElementById("recent-posts");e&&"/"==location.pathname&&(console.log("已挂载github calendar"),e.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>')),GithubCalendar("https://python-github-calendar-api.vercel.app/api?Leekinghou",["#ebedf0","#fdcdec","#fc9bd9","#fa6ac5","#f838b2","#f5089f","#c4067e","#92055e","#540336","#48022f","#30021f"],"Leekinghou")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:0}}</style><style></style></body></html>