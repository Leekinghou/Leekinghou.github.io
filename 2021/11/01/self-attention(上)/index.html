<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Self-attention(上) | lijinhao's blog</title><meta name="keywords" content="ML"><meta name="author" content="lijinhao"><meta name="copyright" content="lijinhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CNN以后学习的另一个架构Self-Attention，常用于自然语言处理，也可以图像处理，核心是用于解决多向量输入的问题"><meta property="og:type" content="article"><meta property="og:title" content="Self-attention(上)"><meta property="og:url" content="https://lijinhao.site/2021/11/01/self-attention(%E4%B8%8A)/index.html"><meta property="og:site_name" content="lijinhao&#39;s blog"><meta property="og:description" content="CNN以后学习的另一个架构Self-Attention，常用于自然语言处理，也可以图像处理，核心是用于解决多向量输入的问题"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111636.png"><meta property="article:published_time" content="2021-10-31T16:00:00.000Z"><meta property="article:modified_time" content="2022-06-20T12:15:35.569Z"><meta property="article:author" content="lijinhao"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Self-attention"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111636.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lijinhao.site/2021/11/01/self-attention(%E4%B8%8A)/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"Copy successfully",error:"Copy error",noSupport:"The browser does not support"},relativeDate:{homepage:!1,post:!1},runtime:"days",date_suffix:{just:"Just",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Self-attention(上)",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-06-20 20:15:35"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));GLOBAL_CONFIG_SITE.isHome&&/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom@1.0.0.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/portrait.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">66</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">39</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">23</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i> <span>清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i> <span>照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">lijinhao's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i> <span>清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i> <span>照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Self-attention(上)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-10-31T16:00:00.000Z" title="Created 2021-11-01 00:00:00">2021-11-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-06-20T12:15:35.569Z" title="Updated 2022-06-20 20:15:35">2022-06-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Self-attention(上)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><p><strong>前言</strong>：当前已经学习的网络结构（感知机、支持向量机、KNN、CNN）的输入都是一个向量，而之前学习的如YouTube观看人数、城市新冠疫情预测等，其输入都可以看作<strong>一个向量</strong>，输出是一个数值时是<strong>回归问题（Regression）</strong>，也可能是一个<strong>类别（Classification）</strong></p><p><strong>question：当输入是多个向量，而且向量的数量会改变时怎么办？</strong>——使用self-attention解决</p><h1 id="Self-attention应用场景"><a href="#Self-attention应用场景" class="headerlink" title="Self-attention应用场景"></a>Self-attention应用场景</h1><ul><li><p>文字处理</p></li><li><p>声音信号处理</p></li><li><p>图</p></li><li><p>分子</p></li></ul><h2 id="文字处理"><a href="#文字处理" class="headerlink" title="文字处理"></a>文字处理</h2><ul><li>输入：一个句子</li><li>每个句子的长度不一样，句子中词汇数目不一样</li></ul><p>句子中的每一个词汇都描述成一个向量，那么model的输入就是一个<strong>向量集</strong>，而这个向量集的大小每次都不一样。</p><p><strong>question：如何将一个词汇表示成向量？</strong></p><h4 id="方法1-One-Hot的Encoding"><a href="#方法1-One-Hot的Encoding" class="headerlink" title="方法1:  One-Hot的Encoding"></a>方法1: One-Hot的Encoding</h4><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111119.png"></p><p>开一个很长的向量，定义每一个单词，很明显这样是不可行的。</p><p>同时这样的表示方法有一个非常严重的问题，它假设所有的<strong>词汇彼此之间都是没有关系</strong>的，从这个向量里面你看不到：Cat跟Dog都是动物所以他们比较接近，Cat跟Apple一个动物一个植物，所以他们比较不相像。这个向量里面，没有任何语义的信息。</p><h4 id="方法2-Word-Embedding"><a href="#方法2-Word-Embedding" class="headerlink" title="方法2: Word Embedding"></a>方法2: Word Embedding</h4><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111134.png"></p><p>Word Embedding就是给每一个词汇一个向量，而这个<strong>向量是有语义的信息的</strong>，看图可以发现所有的动物可能聚集成一团，所有的植物可能聚集成一团，所有的动词可能聚集成一团。至于是怎么让他们分别汇聚的<br>可以看：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=X7PH3NuYW0Q">Youtube:Unsupervised Learning - Word Embedding</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1PW411g7xG?from=search&seid=2710964519834035087&spm_id_from=333.337.0.0">B站:Unsupervised Learning - Word Embedding</a></p><h2 id="声音信号处理"><a href="#声音信号处理" class="headerlink" title="声音信号处理"></a>声音信号处理</h2><ul><li>一段声音信号是一排向量，将一段声音信号取一个范围，这个范围就是一个<strong>window</strong><br><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111147.png"></li></ul><p>把窗口内的信息描述成一个向量，这个向量就叫做<strong>Frame</strong>，通常这个窗口的长度是25毫秒。</p><p>而把声音信号变成向量的方法很多，如<strong>MFCC</strong></p><p>为了要描述一整段的声音信号，把这个<strong>Window往右移一点</strong>，通常移动的大小是<strong>10毫秒</strong>（前人测试所得）</p><ul><li>所以1s的声音有100个向量，所以1min的声音就有100*60=6000个向量。所以语音其实很复杂的，一小段的声音，它里面包含的信息量其实是非常可观的。</li></ul><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><p>一个图，也是一堆向量。例如Social Network这个图：</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111222.png"></p><ul><li>每个人可以看作一个节点，其中的工作、性别、年龄可以看作向量</li></ul><h2 id="分子信息"><a href="#分子信息" class="headerlink" title="分子信息"></a>分子信息</h2><p>识别出分子是否具有毒性、是否亲水……</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111256.png"></p><ul><li><strong>一个分子可以看作是一个Graph</strong>，分子上面的每一个球，也就是<strong>每一个原子，都可以表述成一个向量</strong></li></ul><p>一个<strong>原子可以用One-Hot Vector</strong>，来表示，氢就是1000，碳就是0100，氧是0010，所以一个分子就是一个Graph，它就是一堆向量。</p><h1 id="输出类型"><a href="#输出类型" class="headerlink" title="输出类型"></a>输出类型</h1><ul><li>每一个向量都有一个对应的Label<ul><li>词性判断</li><li>音标标注</li></ul></li><li>一整个序列输入，只需要输出一个Label<ul><li>情感分析，输出positive/negative，optimistic/pessimistic</li></ul></li><li>算法决定输入多少个Label<ul><li>人不知道应该输出多少个Label，机器要自己决定，应该要输出多少个Label，可能你输入是$N$个向量，输出可能是$N’$个Label，为什么是$N’$，机器自己决定。</li><li>这种任务又叫做<strong>sequence to sequence</strong>的任务</li></ul></li></ul><h3 id="序列标注-Sequence-Labeling"><a href="#序列标注-Sequence-Labeling" class="headerlink" title="序列标注 Sequence Labeling"></a>序列标注 Sequence Labeling</h3><p>本篇记录的是输出类型中的第一种情况：输入跟输出数目一样多。这种情况又叫做<strong>序列标注（Sequence Labeling）</strong>。</p><p>在词性识别中：<br>$$<br>I\quad saw\quad a\quad saw(我看到一把锯子)<br>$$<br>如何识别出两个saw的词性？</p><p>可以<strong>把前后几个向量都串起来，一起丢到Fully-Connected的Network中进行识别得出结果</strong>，也就是我们说的，<strong>考虑上下文</strong></p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111315.png"></p><p>但是这样的方法是有局限性的，如果需要考虑整个序列时：</p><ul><li><p>Sequence的长度是有长有短，输入给我们的Model的Sequence的长度，每次可能都不一样</p></li><li><p>或者说建立一个很大的窗口可以包容所有长度的序列，这意味着Fully-Connected的Network需要非常多的参数，那可能不只<strong>运算量很大，可能还容易Overfitting</strong></p></li></ul><p>既然如此，要解决上述问题，就可以考虑本篇要说的<strong>Self-attention技术</strong>了。</p><h1 id="详解Self-attention"><a href="#详解Self-attention" class="headerlink" title="详解Self-attention"></a>详解Self-attention</h1><h2 id="来源和总体思路"><a href="#来源和总体思路" class="headerlink" title="来源和总体思路"></a>来源和总体思路</h2><ul><li>出自<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">《Attention is all you need》</a>，作者首次提到<strong>Transformer</strong></li></ul><p>self-attention的运作方式是一次性接受整个sequence的信息，self-attention输入几个向量，它就输出几个向量。</p><p>对于输出，也就是红色框内的向量，<strong>他们都是考虑一整个Sequence以后才得到</strong>，并不是一个普通的向量。</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111338.png"></p><p>​ 如此一来这个Fully-Connected的Network，就不再只是一个只考虑非常小的范围，或很小一个Window的网络，而是考虑整个Sequence的信息并决定输出什么样的结果，这就是<strong>Self-Attention</strong>，而且<strong>Self-Attention不是只能用一次，可以叠加很多次</strong>。</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111359.png"></p><p><strong>可以把Fully-Connected的Network，跟Self-Attention交替使用</strong></p><ul><li>Self-Attention处理整个Sequence的信息</li><li>Fully-Connected的Network，专注于处理某一个位置的信息</li><li>再用Self-Attention把整个Sequence信息再处理一次</li><li>然后交替使用Self-Attention跟Fully-Connected</li></ul><h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p><strong>Self-Attention的输入，它就是一串的向，这个向量可能整个Network的输入，也可能是某个Hidden Layer的输出。</strong>此处用$a^i$表示</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111418.png"></p><p><strong>question: 从self-attention输入的每一个$b^i$都是考虑了所有$a^i$而得出的，那么生成$b^i$的规则是什么呢？</strong></p><h2 id="计算attention的模组"><a href="#计算attention的模组" class="headerlink" title="计算attention的模组"></a><strong>计算attention的模组</strong></h2><p>对于$a^1$，要从整个序列中找出与其相关性较大的其他向量$a^n$（find the relevant vectors in a sequence）</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111436.png"></p><p>计算每一个向量$a^n$与$a^1$的关联程度，用一个数值$a$来表示，计算关联程度的过程，就是计算attention模组的过程</p><h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>计算attention的模组，就是<strong>输入两个向量</strong>，然后输出$a$的数值。</p><p><strong>计算方法有各种各样的做法（论文学习：dot product、additive等）</strong>，总之<strong>目的就是计算各个向量的关联程度</strong>。</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111456.png"></p><ol><li><p>dot product</p><p>首先，输入的这两个向量分别乘上两个不同的矩阵，左边这个向量乘上$W^q$矩阵得到矩阵$q$，右边的向量乘上$W^k$矩阵得到矩阵$k$，然后把矩阵$q$跟矩阵$k$做dot product，也就是把他们做<strong>element-wise 的相乘</strong>，再全部加起来以后就得到一个 scalar，这个scalar就是矩阵$a$，这是一种计算$a$的方式。</p><p>ps：elementwise multiplication 直白翻译过来就是元素的智能乘积。例如 <img src="https://www.zhihu.com/equation?tex=v%5Codot+w+=+s" alt="[公式]"> 表示对每一个输入向量 $v$ 乘以一个给定的”权重”—— $w$向量。换句话说，就是通过一个乘子对数据集的每一列进行缩放。这个转换可以表示为如下的形式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+v_1,v_2,v_3+%5Cright%5C%7D%5ET%5Codot%5Cleft%5C%7B+w_1,w_2,w_3+%5Cright%5C%7D%5ET=%5Cleft%5C%7B+v_1w_1,v_2w_2,v_3w_3+%5Cright%5C%7D%5ET" alt="[公式]"></p><p>再直白一点，就是同位元素对应相乘。</p></li><li><p>Additive</p><p>首先，把同样这两个向量通过$W^q$， $W^k$得到$q$跟$k$，然后不做Dot-Product，而是把他们串起来，接着丢到Activation Function中，最后通过一个Transform，得到α。</p></li></ol><p>以下采用dot product方法进行，也是用在Transformer里面的方法</p><p>把$a^1$分别与$a^2$、$a^3$、$a^4$计算他们之间的关联性，也就是计算他们之间的$a$，一般实际操作时，$q^1$会自己做query和key计算自己跟自己的关联性。</p><p>$q$ 代表Query，它就像是使用搜寻引擎，去搜寻相关文章时的<strong>关键字</strong>，所以叫做Query。</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111518.png"></p><p>计算出$a^1$跟每一个向量的关联性以后，接下来会<strong>接入一个Soft-Max</strong>，但并不是非soft-max不可，也可以使用别的，例如说ReLU，<strong>总而言之使用什么Activation Function都行，以实际效果为评判标准</strong></p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111529.png"></p><p>Soft-Max的输出就是一排α′，所以本来有一排α，通过Soft-Max就得到α′。</p><h3 id="α′的作用及操作"><a href="#α′的作用及操作" class="headerlink" title="α′的作用及操作"></a>α′的作用及操作</h3><p>求得α的初心是为了知道哪些向量跟$a^1$的关系较大，所以辗转<strong>得到的α′，是用于抽取出Sequence里面相关性高的向量。</strong></p><p>那么具体要如何抽取呢？</p><p><img src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111542.png"></p><ol><li><p>首先把$a^1$到$a^4$（也就是每一个向量）乘上$W^v$得到新的向量，此处用$v^1，v^2，……$，表示</p></li><li><p>接下来把$v^1$到$v^4$每一个向量都乘上经过了激活函数Activation Function之后的attention score分数，也就是α′</p></li><li><p>再做累加，得到一个$b^1$</p></li></ol><p>$$<br>b^1 = \sum_{i}a_{1,i}^`,v^i<br>$$</p><p><strong>如果某一个向量得到的分数越高，比如说$a^1$和$a^2$的关联性很强，那么由$a^1$和$a^2$得到的α′的值自然就会很大，那么得到由各个α′与$v^i$相乘之后相加所得的$b^1$的值，就可能会比较接近于$v^2$，也就是说$v^2$占据了主导地位。</strong></p><p>以上就是从一整个输入序列中求得$b^1$的方法。</p><p>但看完本篇难免会有一个问题：</p><p>文中提到的$W^q 、 W^k 、 W^v$都是什么？又从哪里来？</p><p>具体可看self-attention下篇。</p></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a><a class="post-meta__tags" href="/tags/Self-attention/">Self-attention</a></div><div class="post_share"><div class="social-share" data-image="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211127111636.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/11/03/2021-10-11-%E7%BB%88%E7%AB%AF%E5%88%A9%E5%99%A8-tmux/"><img class="prev-cover" src="https://inews.gtimg.com/newsapp_ls/0/14137230625/0" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">终端利器——tmux</div></div></a></div><div class="next-post pull-right"><a href="/2021/10/26/%E5%89%91%E6%8C%8753/"><img class="next-cover" src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/tempImg/20211124181101.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">剑指 Offer 53 - I. 在排序数组中查找数字 I</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/11/06/self-attention(%E4%B8%8B)/" title="Self-attention(下)"><img class="cover" src="https://inews.gtimg.com/newsapp_ls/0/14146047881/0" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Self-attention(下)</div></div></a></div><div><a href="/2021/09/28/2021-09-28-sklearn%E4%BD%BF%E7%94%A8%E5%87%BD%E6%95%B0%E6%96%87%E6%A1%A3/" title="sklearn使用函数文档"><img class="cover" src="https://files.catbox.moe/nqxbpk.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-28</div><div class="title">sklearn使用函数文档</div></div></a></div><div><a href="/2021/09/22/CNN/" title="CNN卷积神经网络笔记"><img class="cover" src="https://image-20220620.oss-cn-guangzhou.aliyuncs.com/image/20220620220530.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-22</div><div class="title">CNN卷积神经网络笔记</div></div></a></div><div><a href="/2021/08/28/C1/" title="MACHINE LEARNING"><img class="cover" src="https://files.catbox.moe/53u8pl.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-28</div><div class="title">MACHINE LEARNING</div></div></a></div><div><a href="/2021/09/26/K%E8%BF%91%E9%82%BB%20K-nearest%20neighbor/" title="K近邻 K-nearest neighbor"><img class="cover" src="https://files.catbox.moe/yechci.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-26</div><div class="title">K近邻 K-nearest neighbor</div></div></a></div><div><a href="/2021/10/20/lessonOne/" title="李宏毅2021年(春)深度学习 作业一"><img class="cover" src="https://inews.gtimg.com/newsapp_ls/0/14146721068/0" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-20</div><div class="title">李宏毅2021年(春)深度学习 作业一</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/portrait.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">lijinhao</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">66</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">39</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">23</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/leekinghou"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/leekinghou" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lijinhao716@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">醉后不知天在水，满船清梦压星河✨<div class="twopeople"><div class="twopeople"><div class="container" style="height:200px"><canvas class="illo" width="800" height="800" style="max-width:200px;max-height:200px;touch-action:none;width:640px;height:640px"></canvas></div><script src="https://cdn.guole.fun/js/twopeople1.js"></script><script src="https://cdn.guole.fun/js/zdog.dist.js"></script><script id="rendered-js" src="https://cdn.guole.fun/js/twopeople.js"></script><style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Self-attention%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">Self-attention应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E5%AD%97%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">文字处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951-One-Hot%E7%9A%84Encoding"><span class="toc-number">1.1.0.1.</span> <span class="toc-text">方法1: One-Hot的Encoding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952-Word-Embedding"><span class="toc-number">1.1.0.2.</span> <span class="toc-text">方法2: Word Embedding</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A3%B0%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">声音信号处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE"><span class="toc-number">1.3.</span> <span class="toc-text">图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%AD%90%E4%BF%A1%E6%81%AF"><span class="toc-number">1.4.</span> <span class="toc-text">分子信息</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">输出类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8-Sequence-Labeling"><span class="toc-number">2.0.1.</span> <span class="toc-text">序列标注 Sequence Labeling</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%A6%E8%A7%A3Self-attention"><span class="toc-number">3.</span> <span class="toc-text">详解Self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%A5%E6%BA%90%E5%92%8C%E6%80%BB%E4%BD%93%E6%80%9D%E8%B7%AF"><span class="toc-number">3.1.</span> <span class="toc-text">来源和总体思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="toc-number">3.2.</span> <span class="toc-text">实现过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97attention%E7%9A%84%E6%A8%A1%E7%BB%84"><span class="toc-number">3.3.</span> <span class="toc-text">计算attention的模组</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">3.3.1.</span> <span class="toc-text">计算过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%CE%B1%E2%80%B2%E7%9A%84%E4%BD%9C%E7%94%A8%E5%8F%8A%E6%93%8D%E4%BD%9C"><span class="toc-number">3.3.2.</span> <span class="toc-text">α′的作用及操作</span></a></li></ol></li></ol></li></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By lijinhao</div><div class="framework-info"><span>Framework</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">满腔期待 常怀热情 有人等你<a target="_blank" rel="noopener" href="https://inews.gtimg.com/newsapp_ls/0/14146688226/0">Baikal♥️Swan</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"></div><script defer id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="false" data-click="false"></script><script src="https://cdn-1.nesxc.com/js/smooth-scrolling.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script><script data-pjax>function GithubCalendarConfig(){var e=document.getElementById("recent-posts");e&&"/"==location.pathname&&(console.log("已挂载github calendar"),e.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>')),GithubCalendar("https://python-github-calendar-api.vercel.app/api?Leekinghou",["#ebedf0","#fdcdec","#fc9bd9","#fa6ac5","#f838b2","#f5089f","#c4067e","#92055e","#540336","#48022f","#30021f"],"Leekinghou")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:0}}</style><style></style></body></html>