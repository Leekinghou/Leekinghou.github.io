<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Scrapy框架的使用 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="2021.4.8 投了一个python爬虫岗位 呼～希望能入职😭">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy框架的使用">
<meta property="og:url" content="http://example.com/2021/04/09/Scrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="2021.4.8 投了一个python爬虫岗位 呼～希望能入职😭">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13390884437/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13391484206/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13391528612/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13391876413/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13392247059/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13392814433/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13392818532/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13394179487/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13394185112/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13394187430/0">
<meta property="og:image" content="https://inews.gtimg.com/newsapp_ls/0/13394194836/0">
<meta property="article:published_time" content="2021-04-09T06:33:15.000Z">
<meta property="article:modified_time" content="2021-04-20T09:27:53.000Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Python网络爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://inews.gtimg.com/newsapp_ls/0/13390884437/0">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Scrapy框架的使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/09/Scrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2021-04-09T06:33:15.000Z" itemprop="datePublished">2021-04-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">网络爬虫</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Scrapy框架的使用
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>2021.4.8 投了一个python爬虫岗位 呼～<br>希望能入职😭</p>
<span id="more"></span>

<h1 id="Scrapy框架的使用"><a href="#Scrapy框架的使用" class="headerlink" title="Scrapy框架的使用"></a>Scrapy框架的使用</h1><p>之前学习了pyspider框架的用法，可以利用它快速完成爬虫的编写。但是pyspider框架自身存在一些缺点，比如说：  </p>
<ol>
<li>可配置程度低</li>
<li>异常处理能力有限</li>
<li>对于强反爬网站力不从心</li>
</ol>
<p>而Scrapy框架更加强大， 爬取效率高，扩展组件非常多，可配置、可扩展程度高，应用也最广泛。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Scrapy基于Twisted的异步处理框架，是纯Python实现的，架构清晰，模块之间耦合度低，非常灵活。同时非常轻便，只需要定制开发几个模块就可以轻松实现一个爬虫。通过多个组件之间的相互协作，不同组件完成各自的任务，以及组件对异步的支持，Scrapy最大限度地提高了网络带宽，提高了数据爬取效率</p>
<p><img src="https://inews.gtimg.com/newsapp_ls/0/13390884437/0" alt="Scrapy架构"></p>
<ul>
<li><p>Spiders<br>定义了爬取的逻辑和网页中抽取的项目，主要负责解析响应并生成提取结果和新的请求</p>
</li>
<li><p>Spider Middlewares<br>中间件，主要处理爬虫输入的响应和输出的结果以及新的请求</p>
</li>
<li><p>Engine<br>处理整个系统的数据流、触发事务，是框架的核心</p>
</li>
<li><p>Scheduler<br>调度器，接受引擎发送的请求并将其加入队列，在引擎再次请求时提供该请求给下载器</p>
</li>
<li><p>Downloader<br>下载网页内容，并将网页内容返回给爬虫</p>
</li>
<li><p>Downloader Middlewares<br>主要处理引擎与下载器之间的请求和响应</p>
</li>
<li><p>Item Pipeline<br>负责处理爬虫从网页中抽取的内容，清洗、验证和存储数据</p>
</li>
<li><p>Item<br>定义了爬取结果的数据结构，爬取的数据会被赋值成该Item对象</p>
</li>
</ul>
<h2 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h2><ol>
<li>Engine首先打开一个网站，找到处理该网站的Spider。</li>
<li>Engine从Spider获取第一个要爬取的URL，并通过Scheduler以Request的形式调度</li>
<li>Engine向Scheduler请求下一个要爬取的URL</li>
<li>Scheduler返回下一个要爬取的URL给Engine，后者将该URL通过Downloader Middlewares发送给Downloader</li>
<li>一旦页面下载完毕，Downloader生成该页面的Response，并将其通过Downloader Middlewares发送给Engine</li>
<li>Engine从下载器中接收到Response，并将其通过Spider Middlewares发送给Spider处理</li>
<li>Spider处理Response，并返回爬取到的Item及新的Request给Engine</li>
<li>Engine将Spider返回的Item给Item Pipeline，将新的Request给Scheduler</li>
<li>重复第2～第8步，直到Scheduler中没有更多的Request，Engine关闭该网站，爬取结束</li>
</ol>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code> pip3 install Scrapy</code></p>
<h3 id="初始化Scrapy项目"><a href="#初始化Scrapy项目" class="headerlink" title="初始化Scrapy项目"></a>初始化Scrapy项目</h3><p>初始化一个名为tutorial的scrapy项目:<br><code>scrapy startproject tutorial</code></p>
<p>此时tuorial的目录结构：<br><img src="https://inews.gtimg.com/newsapp_ls/0/13391484206/0"></p>
<h3 id="创建爬虫"><a href="#创建爬虫" class="headerlink" title="创建爬虫"></a>创建爬虫</h3><p>新建一个名为quotes的爬虫爬取quotes.toscrape.com这个网站：<br><code> scrapy genspider quotes quotes.toscrape.com</code></p>
<p><img src="https://inews.gtimg.com/newsapp_ls/0/13391528612/0"></p>
<p>查看quotes.py的内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;quotes.toscrape.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到有三个属性：name、allowed_domains、start_urls和一个方法parse  </p>
<ul>
<li>name是每个项目唯一的名字</li>
<li>allowed_domains是目标网站</li>
<li>start_urls是spider启动时爬取的url列表，初始请求由它来定义</li>
<li>parse：默认情况下，被调用时start_urls里面的链接构成的请求完成下载执行之后，返回的响应就会作为唯一的参数传递给这个函数。parse函数负责解析返回的响应、提取数据或者进一步生成要处理的请求。</li>
</ul>
<h3 id="创建Item"><a href="#创建Item" class="headerlink" title="创建Item"></a>创建Item</h3><p>Item时保存爬取数据的容器，使用方法与字典类似。</p>
<p>但相比字典多了额外的保护机制，可以避免拼写错误或者定义字段错误</p>
<p>创建Item需要继承scrapy.Item类，并且定义scrapy.Field字段。一般可以获取到的内容有text、author、tags。</p>
<p>定义Item时将items.py修改如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TutorialItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    text = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    author = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    tags = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h3 id="解析Response"><a href="#解析Response" class="headerlink" title="解析Response"></a>解析Response</h3><p>前面的代码可以看到parse()方法的参数response时start_urls里面的链接爬取后的结果。</p>
<p>所以在parse()方法中，我们可以直接对response变量包含的内容进行解析，从中可以得到网页源代码，找出结果中的链接而得到下一个请求。</p>
<p><img src="https://inews.gtimg.com/newsapp_ls/0/13391876413/0" alt="网页结构"></p>
<p>查看<a target="_blank" rel="noopener" href="http://quotes.toscrape.com/">http://quotes.toscrape.com/</a><br>其网页结构可以发现，每一页都是由多个class为quote的区块构成，而每个区块又包含text、author、tags。那么可以先找出quote然后提取出里面的内容</p>
<h3 id="后续Request"><a href="#后续Request" class="headerlink" title="后续Request"></a>后续Request</h3><p>上面的操作实现了从初始页面抓取内容。下一页的内容由上一页抓取的内容构建下一个请求获得。</p>
<p><img src="https://inews.gtimg.com/newsapp_ls/0/13392247059/0"></p>
<p>底部的Next按钮查看源代码后可以发现链接时/page/2，全链接就是：<a target="_blank" rel="noopener" href="https://quotes.toscrape.com/page/2">https://quotes.toscrape.com/page/2</a><br>通过这个链接可以构建下一个请求</p>
<ul>
<li><p>scrapy.Request<br>构造请求时需要用到scrapy.Request,这里需要传递两个参数url、callback</p>
<ul>
<li>url: 要请求的链接</li>
<li>callback: 回调函数。当指定了该回调函数的请求完成之后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。回调函数进行解析或生成下一个请求，回调函数如parse()所示。</li>
</ul>
</li>
<li><p>实际上parse()就是解析text、author、tags的方法。下一页的结构和第一页已经解析过的页面结构时一样的，所以可以复用代码。</p>
</li>
</ul>
<p>利用选择器得到下一页链接并生成请求，在parse()方法后追加如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span> = response.css(<span class="string">&#x27;.pager .next a::attr(&quot;href&quot;)&#x27;</span>).extract_first()</span><br><span class="line">url = response.urljoin(<span class="built_in">next</span>)</span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(url=url,callback = self.parse)</span><br></pre></td></tr></table></figure>

<p>第一句代码首先通过CSS选择器获取下一个页面的链接，即要获取a超链接中的href属性。这里用到了::attr(href)操作。然后再调用extract_first方法获取内容。</p>
<p>第二句代码调用了urljoin()方法,urljoin()方法可以将相对URL构造成绝对url。</p>
<p>第三句代码通过url和callback变量构造了一个新的请求，回调函数callback依然使用parse()方法。这个请求完成后，响应会重新经过parse方法处理，得到第二页的解析结果。</p>
<p>改写后的Spider类如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> TutorialItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;quotes.toscrape.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">            item = TutorialItem()</span><br><span class="line">            item[<span class="string">&#x27;text&#x27;</span>] = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;author&#x27;</span>] = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;tags&#x27;</span>] = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">            </span><br><span class="line">        <span class="built_in">next</span> = response.css(<span class="string">&#x27;.pager .next a::attr(&quot;href&quot;)&#x27;</span>).extract_first()</span><br><span class="line">        url = response.urljoin(<span class="built_in">next</span>)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=url,callback = self.parse)</span><br></pre></td></tr></table></figure>

<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p><code>scrapy crawl quotes</code></p>
<p><img src="https://inews.gtimg.com/newsapp_ls/0/13392814433/0"></p>
<p>scrapy首先会输出当前版本信息和正在启动的项目和一些配置情况</p>
<ul>
<li>抓取的数据<br><img src="https://inews.gtimg.com/newsapp_ls/0/13392818532/0"></li>
</ul>
<h3 id="保存文件"><a href="#保存文件" class="headerlink" title="保存文件"></a>保存文件</h3><ul>
<li>将结果保存为JSON文件</li>
</ul>
<p><code>scrapy crawl quotes -o quotes.json</code></p>
<p>每一个Item输出一行JSON，输出后缀为jl，为jsonline的缩写：</p>
<p><code>scrapy crawl quotes -o quotes.jl</code></p>
<p>或：</p>
<p><code>scrapy crawl quotes -o quotes.jsonlines</code></p>
<p>输出格式还支持很多种，例如csv、xml、pickle、marshal等，还支持ftp、s3等远程传输，还可以通过自定义ItemExporter实现其他的输出</p>
<p><code>scrapy crawl quotes -o quotes.csv</code><br><code>scrapy crawl quotes -o quotes.xml</code><br><code>scrapy crawl quotes -o quotes.pickle</code><br><code>scrapy crawl quotes -o quotes.marshal</code><br><code>scrapy crawl quotes -o ftp://user:pass@ftp.example.com/path/to/quotes.csv</code>  </p>
<p>如果要输出到数据库，需要使用Item Pileline</p>
<h2 id="使用Item-Pipeline"><a href="#使用Item-Pipeline" class="headerlink" title="使用Item Pipeline"></a>使用Item Pipeline</h2><p>负责处理爬虫从网页中抽取的内容，清洗、验证和存储数据</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬取数据，检查爬取字段</li>
<li>查重并且丢弃重复数据</li>
<li>将爬取的结果保存到数据库</li>
</ul>
<h3 id="使用方法-1"><a href="#使用方法-1" class="headerlink" title="使用方法"></a>使用方法</h3><p>要实现Item Pipeline只需要定义一个类并实现process_item()方法即可。启用Item Pipeline后，Item Pipeline会自动调用这个方法。process_item方法必须返回包含数据的字典或Item对象，或者抛出DropItem异常</p>
<p>process_item()方法只有两个参数：</p>
<ul>
<li>item：由Spider生成的Item，相当于“原材料”</li>
<li>spider：就是Spider实例</li>
</ul>
<p>步骤：</p>
<ul>
<li><p>修改项目中的pipelines.py<br>删除用命令行自动生成的文件内容，增加一个TextPipeline类，内容如下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TutorialPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 定义限制长度为50</span></span><br><span class="line">        self.limit = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="comment"># 判断item的text属性是否存在，不存在则抛出DropItem异常</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line">            <span class="comment"># 大于50就截断然后拼接省略号，再将item返回</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&#x27;text&#x27;</span>]) &gt; self.limit:</span><br><span class="line">                item[<span class="string">&#x27;text&#x27;</span>] = item[<span class="string">&#x27;text&#x27;</span>][<span class="number">0</span>:self.limit].rstrip() + <span class="string">&#x27;…&#x27;</span></span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> DropItem(<span class="string">&#x27;Missing Text&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>存入数据库</p>
</li>
</ul>
<p>将处理好的item——“加工后的材料”存入MongoDB，需要定义另一个Pipeline，同样在<code>pipelines.py</code>里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mongo_uri, mongo_db</span>):</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用@staticmethod或@classmethod，可以不需要实例化，直接类名.方法名()来调用</span></span><br><span class="line">    <span class="comment"># cls作为第一个参数用来表示类本身</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="comment"># 用于获取setting中的全局配置</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri = crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">&#x27;MONGO_DB&#x27;</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主要是做初始化操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 核心，执行数据的插入操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        self.db[name].insert(<span class="built_in">dict</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭数据库连接</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure>

<ul>
<li>修改setting.py</li>
</ul>
<p>添加以下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fuzhi ITEM_PIPELINES字典，键名是Pipeline的类名称，键值是调用优先级，数字越小对应的Pipeline越先调用</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;tutoial.pipelines.TextPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;tutoial.pipelines.MongoPipleline&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中&lt;username&gt;和&lt;password&gt;要修改为实际的用户名和密码</span></span><br><span class="line">MONGO_URI = <span class="string">&#x27;mongodb+srv://&lt;username&gt;:&lt;password&gt;@cluster0.yker2.mongodb.net/myFirstDatabase?retryWrites=true&amp;w=majority&#x27;</span></span><br><span class="line"></span><br><span class="line">MONGO_DB = <span class="string">&#x27;tutorial&#x27;</span></span><br></pre></td></tr></table></figure>
<p>本次爬取的数据存入的事是Mongodb数据库，用的是Mongodb提供的免费的网络集群，使用方法可以自行百度<br><img src="https://inews.gtimg.com/newsapp_ls/0/13394179487/0"><br>点击CONNECT选择对应的python版本可以获得对应的MONGO_URI<br><img src="https://inews.gtimg.com/newsapp_ls/0/13394185112/0"><br>将其复制并修改对应的密码即可<br><img src="https://inews.gtimg.com/newsapp_ls/0/13394187430/0"></p>
<ul>
<li>启动爬取<br><code>scrapy crawl quotes</code><br>我下载了Mongodb的GUI，可以看到对应的结果为：<br><img src="https://inews.gtimg.com/newsapp_ls/0/13394194836/0"></li>
</ul>
<p>完整代码在Github: <a href="url">https://github.com/Leekinghou/tutorial</a></p>
<p>2021.4.12 面试顺利通过了，开局一张笔试题主要是问python特性：</p>
<ol start="0">
<li>python的特点</li>
<li>迭代器、生成器、可迭代对象和应用场景 + 解释装饰器</li>
<li>数据类型，可变/不可变类型</li>
<li>*args、**kwargs的区别</li>
<li>两段代码的区别</li>
<li>手撕排序算法</li>
<li>赋值、浅拷贝、深拷贝</li>
<li>map和zip的区别</li>
<li>其他</li>
</ol>
<p>面试询问：  </p>
<ol>
<li>Scrapy框架的组成、数据流动流程</li>
<li>反爬机制、应对方法</li>
<li>遇到过最有难度的目标网站</li>
<li>对OpenCV的了解(简历中的项目)</li>
<li>对Mysql、redis、Mongodb的应用</li>
<li>对Item pipeline的了解</li>
<li>对事务的了解</li>
<li>左连接是什么？</li>
</ol>
<p>但是只开6k，租房吃饭不剩多少钱了🤦‍♂️<br>不去了叭。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/09/Scrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8/" data-id="cktuzinrh000ld2oqcdgf7jwa" data-title="Scrapy框架的使用" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" rel="tag">Python网络爬虫</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/04/10/Scrapy%E7%BB%84%E4%BB%B61/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Scrapy组件（1）
        
      </div>
    </a>
  
  
    <a href="/2021/04/09/I-O%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">I/O输入输出</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Git%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">Git使用方法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/">搭建博客</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">网络爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git%E5%9F%BA%E7%A1%80/" rel="tag">Git基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/" rel="tag">Java语言基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" rel="tag">Python网络爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/" rel="tag">java语言基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python%E9%97%AE%E9%A2%98/" rel="tag">python问题</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91-%E7%88%AC%E8%99%AB/" rel="tag">数据开发 爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Git%E5%9F%BA%E7%A1%80/" style="font-size: 10px;">Git基础</a> <a href="/tags/Java%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/" style="font-size: 20px;">Java语言基础</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" style="font-size: 20px;">Python网络爬虫</a> <a href="/tags/java%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/" style="font-size: 10px;">java语言基础</a> <a href="/tags/python%E9%97%AE%E9%A2%98/" style="font-size: 10px;">python问题</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91-%E7%88%AC%E8%99%AB/" style="font-size: 10px;">数据开发 爬虫</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/08/22/4%E6%9C%88-8%E6%9C%88-%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%8E%86/">数据开发岗位实习经历</a>
          </li>
        
          <li>
            <a href="/2021/04/22/python-pycurl%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/">python-pycurl报错问题</a>
          </li>
        
          <li>
            <a href="/2021/04/21/Git-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">Git 使用方法</a>
          </li>
        
          <li>
            <a href="/2021/04/20/Python%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E5%9B%9E%E9%A1%BE/">Python基本语法回顾</a>
          </li>
        
          <li>
            <a href="/2021/04/14/%E9%9B%B6%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/">零基础使用hexo搭建博客</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>